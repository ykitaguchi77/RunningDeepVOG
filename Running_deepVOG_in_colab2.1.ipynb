{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/deepVOG/blob/master/Running_deepVOG_in_colab2.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTBpmxwwsGiG",
        "colab_type": "text"
      },
      "source": [
        "#Running Github codes in Google Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h56ThViusBCy",
        "colab_type": "code",
        "outputId": "634f66d2-cf73-4233-93c1-9ff21feaf528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "#Install requirements\n",
        "!pip install scikit-video\n",
        "!pip install scikit-image\n",
        "!pip install tensorflow==1.14.0\n",
        "import keras"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-video in /usr/local/lib/python3.6/dist-packages (1.1.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-video) (1.18.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from scikit-video) (7.0.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (0.16.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.4.1)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (3.2.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (7.0.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image) (2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio>=2.3.0->scikit-image) (1.18.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.7)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image) (4.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.12.0)\n",
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ2cvzdNu3o2",
        "colab_type": "text"
      },
      "source": [
        "#Google driveをマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Eh2rquETvh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea7ccbf0-9a2e-4051-c15f-7d7ed5b13999"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JEtf_bFo5F",
        "colab_type": "text"
      },
      "source": [
        "#__init__py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awujmbwLFpTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import Ellipse\n",
        "from skimage.measure import label, regionprops\n",
        "from skimage.morphology import closing, square\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from skimage.draw import ellipse_perimeter, line, circle_perimeter, line_aa\n",
        "import skvideo.io as skv\n",
        "import pdb\n",
        "import os\n",
        "import warnings\n",
        "import traceback\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoC9levwF5WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bwperim.py\n",
        "# vim: set ts=4 sts=4 sw=4 expandtab smartindent:\n",
        "#\n",
        "# This file was originally part of the octave-forge project\n",
        "# Ported to python by Luis Pedro Coelho <luis@luispedro.org> (February 2008)\n",
        "# Copyright (C) 2006       Soren Hauberg\n",
        "# Copyright (C) 2008-2010  Luis Pedro Coelho (Python port)\n",
        "# \n",
        "# This program is free software; you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation; either version 2, or (at your option)\n",
        "# any later version.\n",
        "# \n",
        "# This program is distributed in the hope that it will be useful, but\n",
        "# WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
        "# General Public License for more details. \n",
        "# \n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this file.  If not, see <http://www.gnu.org/licenses/>.\n",
        "\n",
        "def bwperim(bw, n=4):\n",
        "    \"\"\"\n",
        "    perim = bwperim(bw, n=4)\n",
        "    Find the perimeter of objects in binary images.\n",
        "    A pixel is part of an object perimeter if its value is one and there\n",
        "    is at least one zero-valued pixel in its neighborhood.\n",
        "    By default the neighborhood of a pixel is 4 nearest pixels, but\n",
        "    if `n` is set to 8 the 8 nearest pixels will be considered.\n",
        "    Parameters\n",
        "    ----------\n",
        "      bw : A black-and-white image\n",
        "      n : Connectivity. Must be 4 or 8 (default: 8)\n",
        "    Returns\n",
        "    -------\n",
        "      perim : A boolean image\n",
        "    \"\"\"\n",
        "\n",
        "    if n not in (4,8):\n",
        "        raise ValueError('mahotas.bwperim: n must be 4 or 8')\n",
        "    rows,cols = bw.shape\n",
        "\n",
        "    # Translate image by one pixel in all directions\n",
        "    north = np.zeros((rows,cols))\n",
        "    south = np.zeros((rows,cols))\n",
        "    west = np.zeros((rows,cols))\n",
        "    east = np.zeros((rows,cols))\n",
        "\n",
        "    north[:-1,:] = bw[1:,:]\n",
        "    south[1:,:]  = bw[:-1,:]\n",
        "    west[:,:-1]  = bw[:,1:]\n",
        "    east[:,1:]   = bw[:,:-1]\n",
        "    idx = (north == bw) & \\\n",
        "          (south == bw) & \\\n",
        "          (west  == bw) & \\\n",
        "          (east  == bw)\n",
        "    if n == 8:\n",
        "        north_east = np.zeros((rows, cols))\n",
        "        north_west = np.zeros((rows, cols))\n",
        "        south_east = np.zeros((rows, cols))\n",
        "        south_west = np.zeros((rows, cols))\n",
        "        north_east[:-1, 1:]   = bw[1:, :-1]\n",
        "        north_west[:-1, :-1] = bw[1:, 1:]\n",
        "        south_east[1:, 1:]     = bw[:-1, :-1]\n",
        "        south_west[1:, :-1]   = bw[:-1, 1:]\n",
        "        idx &= (north_east == bw) & \\\n",
        "               (south_east == bw) & \\\n",
        "               (south_west == bw) & \\\n",
        "               (north_west == bw)\n",
        "    return ~idx * bw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdNnuKhRNHBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#draw_ellipse.py\n",
        "def isolate_islands(prediction, threshold):\n",
        "    bw = closing(prediction > threshold , square(3))\n",
        "    labelled = label(bw)  \n",
        "    regions_properties = regionprops(labelled)\n",
        "    max_region_area = 0\n",
        "    select_region = 0\n",
        "    for region in regions_properties:\n",
        "        if region.area > max_region_area:\n",
        "            max_region_area = region.area\n",
        "            select_region = region\n",
        "    output = np.zeros(labelled.shape)\n",
        "    if select_region == 0:\n",
        "        return output\n",
        "    else:\n",
        "        output[labelled == select_region.label] = 1\n",
        "        return output\n",
        "\n",
        "# input: output from bwperim -- 2D image with perimeter of the ellipse = 1\n",
        "def gen_ellipse_contour_perim(perim, color = \"r\"): \n",
        "    # Vertices\n",
        "    input_points = np.where(perim == 1)\n",
        "    if (np.unique(input_points[0]).shape[0]) < 6 or (np.unique(input_points[1]).shape[0]< 6) :\n",
        "        return None\n",
        "    else:\n",
        "        try:\n",
        "            vertices = np.array([input_points[0], input_points[1]]).T\n",
        "            # Contour\n",
        "            fitted = LSqEllipse()\n",
        "            fitted.fit([vertices[:,1], vertices[:,0]])\n",
        "            center, w,h, radian = fitted.parameters()\n",
        "            ell = mpl.patches.Ellipse(xy = [center[0],center[1]], width = w*2, height = h*2, angle = np.rad2deg(radian), fill = False, color = color)\n",
        "            # Because of the np indexing of y-axis, orientation needs to be minus\n",
        "            rr, cc = ellipse_perimeter(int(np.round(center[0])), int(np.round(center[1])), int(np.round(w)), int(np.round(h)), -radian)\n",
        "            return (rr, cc, center, w, h, radian, ell)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def gen_ellipse_contour_perim_compact(perim): \n",
        "    # Vertices\n",
        "    input_points = np.where(perim == 1)\n",
        "    if (np.unique(input_points[0]).shape[0]) < 6 or (np.unique(input_points[1]).shape[0]< 6) :\n",
        "        return None\n",
        "    else:\n",
        "        try:\n",
        "            vertices = np.array([input_points[0], input_points[1]]).T\n",
        "            # Contour\n",
        "            fitted = LSqEllipse()\n",
        "            fitted.fit([vertices[:,1], vertices[:,0]])\n",
        "            center, w,h, radian = fitted.parameters()\n",
        "            # Because of the np indexing of y-axis, orientation needs to be minus\n",
        "            return (center, w,h, radian)\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "def fit_ellipse(img, threshold = 0.5, color = \"r\", mask=None):\n",
        "\n",
        "    isolated_pred = isolate_islands(img, threshold = threshold)\n",
        "    perim_pred = bwperim(isolated_pred)\n",
        "\n",
        "    # masking eyelid away from bwperim_output. Currently not available in DeepVOG (But will be used in DeepVOG-3D)\n",
        "    if mask is not None:\n",
        "        mask_bool = mask < 0.5\n",
        "        perim_pred[mask_bool] = 0\n",
        "\n",
        "    # masking bwperim_output on the img boundaries as 0 \n",
        "    perim_pred[0, :] = 0\n",
        "    perim_pred[perim_pred.shape[0]-1, :] = 0\n",
        "    perim_pred[:, 0] = 0\n",
        "    perim_pred[:, perim_pred.shape[1]-1] = 0\n",
        "    ellipse_info = gen_ellipse_contour_perim(perim_pred, color)\n",
        "\n",
        "    return ellipse_info\n",
        "\n",
        "def fit_ellipse_compact(img, threshold = 0.5, mask=None):\n",
        "    \"\"\"Fitting an ellipse to the thresholded pixels which form the largest connected area.\n",
        "    Args:\n",
        "        img (2D numpy array): Prediction from the DeepVOG network (240, 320), float [0,1]\n",
        "        threshold (scalar): thresholding pixels for fitting an ellipse\n",
        "        mask (2D numpy array): Prediction from DeepVOG-3D network for eyelid region (240, 320), float [0,1].\n",
        "                                intended for masking away the eyelid such as the fitting is better\n",
        "    Returns:\n",
        "        ellipse_info (tuple): A tuple of (center, w, h, radian), center is a list [x-coordinate, y-coordinate] of the ellipse centre. \n",
        "                                None is returned if no ellipse can be found.\n",
        "    \"\"\"\n",
        "    isolated_pred = isolate_islands(img, threshold = threshold)\n",
        "    perim_pred = bwperim(isolated_pred)\n",
        "\n",
        "    # masking eyelid away from bwperim_output. Currently not available in DeepVOG (But will be used in DeepVOG-3D)\n",
        "    if mask is not None:\n",
        "        mask_bool = mask < 0.5\n",
        "        perim_pred[mask_bool] = 0\n",
        "\n",
        "    # masking bwperim_output on the img boundaries as 0 \n",
        "    perim_pred[0, :] = 0\n",
        "    perim_pred[perim_pred.shape[0]-1, :] = 0\n",
        "    perim_pred[:, 0] = 0\n",
        "    perim_pred[:, perim_pred.shape[1]-1] = 0\n",
        "    \n",
        "    ellipse_info = gen_ellipse_contour_perim_compact(perim_pred)\n",
        "    return ellipse_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMjmpyShNwq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ellipses.py\n",
        "\"\"\"Demonstration of least-squares fitting of ellipses\n",
        "    __author__ = \"Ben Hammel, Nick Sullivan-Molina\"\n",
        "    __credits__ = [\"Ben Hammel\", \"Nick Sullivan-Molina\"]\n",
        "    __maintainer__ = \"Ben Hammel\"\n",
        "    __email__ = \"bdhammel@gmail.com\"\n",
        "    __status__ = \"Development\"\n",
        "    Requirements \n",
        "    ------------\n",
        "    Python 2.X or 3.X\n",
        "    numpy\n",
        "    matplotlib\n",
        "    References\n",
        "    ----------\n",
        "    (*) Halir, R., Flusser, J.: 'Numerically Stable Direct Least Squares \n",
        "        Fitting of Ellipses'\n",
        "    (**) http://mathworld.wolfram.com/Ellipse.html\n",
        "    (***) White, A. McHale, B. 'Faraday rotation data analysis with least-squares \n",
        "        elliptical fitting'\n",
        "\"\"\"\n",
        "\n",
        "class LSqEllipse:\n",
        "\n",
        "    def fit(self, data):\n",
        "        \"\"\"Lest Squares fitting algorithm \n",
        "        Theory taken from (*)\n",
        "        Solving equation Sa=lCa. with a = |a b c d f g> and a1 = |a b c> \n",
        "            a2 = |d f g>\n",
        "        Args\n",
        "        ----\n",
        "        data (list:list:float): list of two lists containing the x and y data of the\n",
        "            ellipse. of the form [[x1, x2, ..., xi],[y1, y2, ..., yi]]\n",
        "        Returns\n",
        "        ------\n",
        "        coef (list): list of the coefficients describing an ellipse\n",
        "           [a,b,c,d,f,g] corresponding to ax**2+2bxy+cy**2+2dx+2fy+g\n",
        "        \"\"\"\n",
        "        x, y = numpy.asarray(data, dtype=float)\n",
        "\n",
        "        #Quadratic part of design matrix [eqn. 15] from (*)\n",
        "        D1 = numpy.mat(numpy.vstack([x**2, x*y, y**2])).T\n",
        "        #Linear part of design matrix [eqn. 16] from (*)\n",
        "        D2 = numpy.mat(numpy.vstack([x, y, numpy.ones(len(x))])).T\n",
        "        \n",
        "        #forming scatter matrix [eqn. 17] from (*)\n",
        "        S1 = D1.T*D1\n",
        "        S2 = D1.T*D2\n",
        "        S3 = D2.T*D2  \n",
        "        \n",
        "        #Constraint matrix [eqn. 18]\n",
        "        C1 = numpy.mat('0. 0. 2.; 0. -1. 0.; 2. 0. 0.')\n",
        "\n",
        "        #Reduced scatter matrix [eqn. 29]\n",
        "        M=C1.I*(S1-S2*S3.I*S2.T)\n",
        "\n",
        "        #M*|a b c >=l|a b c >. Find eigenvalues and eigenvectors from this equation [eqn. 28]\n",
        "        eval, evec = numpy.linalg.eig(M) \n",
        "\n",
        "        # eigenvector must meet constraint 4ac - b^2 to be valid.\n",
        "        cond = 4*numpy.multiply(evec[0, :], evec[2, :]) - numpy.power(evec[1, :], 2)\n",
        "        a1 = evec[:, numpy.nonzero(cond.A > 0)[1]]\n",
        "        \n",
        "        #|d f g> = -S3^(-1)*S2^(T)*|a b c> [eqn. 24]\n",
        "        a2 = -S3.I*S2.T*a1\n",
        "        \n",
        "        # eigenvectors |a b c d f g> \n",
        "        self.coef = numpy.vstack([a1, a2])\n",
        "        self._save_parameters()\n",
        "            \n",
        "    def _save_parameters(self):\n",
        "        \"\"\"finds the important parameters of the fitted ellipse\n",
        "        \n",
        "        Theory taken form http://mathworld.wolfram\n",
        "        Args\n",
        "        -----\n",
        "        coef (list): list of the coefficients describing an ellipse\n",
        "           [a,b,c,d,f,g] corresponding to ax**2+2bxy+cy**2+2dx+2fy+g\n",
        "        Returns\n",
        "        _______\n",
        "        center (List): of the form [x0, y0]\n",
        "        width (float): major axis \n",
        "        height (float): minor axis\n",
        "        phi (float): rotation of major axis form the x-axis in radians \n",
        "        \"\"\"\n",
        "\n",
        "        #eigenvectors are the coefficients of an ellipse in general form\n",
        "        #a*x^2 + 2*b*x*y + c*y^2 + 2*d*x + 2*f*y + g = 0 [eqn. 15) from (**) or (***)\n",
        "        a = self.coef[0,0]\n",
        "        b = self.coef[1,0]/2.\n",
        "        c = self.coef[2,0]\n",
        "        d = self.coef[3,0]/2.\n",
        "        f = self.coef[4,0]/2.\n",
        "        g = self.coef[5,0]\n",
        "        \n",
        "        #finding center of ellipse [eqn.19 and 20] from (**)\n",
        "        x0 = (c*d-b*f)/(b**2.-a*c)\n",
        "        y0 = (a*f-b*d)/(b**2.-a*c)\n",
        "        \n",
        "        #Find the semi-axes lengths [eqn. 21 and 22] from (**)\n",
        "        numerator = 2*(a*f*f+c*d*d+g*b*b-2*b*d*f-a*c*g)\n",
        "        denominator1 = (b*b-a*c)*( (c-a)*numpy.sqrt(1+4*b*b/((a-c)*(a-c)))-(c+a))\n",
        "        denominator2 = (b*b-a*c)*( (a-c)*numpy.sqrt(1+4*b*b/((a-c)*(a-c)))-(c+a))\n",
        "        width = numpy.sqrt(numerator/denominator1)\n",
        "        height = numpy.sqrt(numerator/denominator2)\n",
        "\n",
        "        # angle of counterclockwise rotation of major-axis of ellipse to x-axis [eqn. 23] from (**)\n",
        "        # or [eqn. 26] from (***).\n",
        "        phi = .5*numpy.arctan((2.*b)/(a-c))\n",
        "\n",
        "        self._center = [x0, y0]\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._phi = phi\n",
        "\n",
        "    @property\n",
        "    def center(self):\n",
        "        return self._center\n",
        "\n",
        "    @property\n",
        "    def width(self):\n",
        "        return self._width\n",
        "\n",
        "    @property\n",
        "    def height(self):\n",
        "        return self._height\n",
        "\n",
        "    @property\n",
        "    def phi(self):\n",
        "        \"\"\"angle of counterclockwise rotation of major-axis of ellipse to x-axis \n",
        "        [eqn. 23] from (**)\n",
        "        \"\"\"\n",
        "        return self._phi\n",
        "\n",
        "    def parameters(self):\n",
        "        return self._center, self._width, self._height, self._phi\n",
        "\n",
        "\n",
        "def make_test_ellipse(center=[1,1], width=1, height=.6, phi=3.14/5):\n",
        "    \"\"\"Generate Elliptical data with noise\n",
        "    \n",
        "    Args\n",
        "    ----\n",
        "    center (list:float): (<x_location>, <y_location>)\n",
        "    width (float): semimajor axis. Horizontal dimension of the ellipse (**)\n",
        "    height (float): semiminor axis. Vertical dimension of the ellipse (**)\n",
        "    phi (float:radians): tilt of the ellipse, the angle the semimajor axis\n",
        "        makes with the x-axis \n",
        "    Returns\n",
        "    -------\n",
        "    data (list:list:float): list of two lists containing the x and y data of the\n",
        "        ellipse. of the form [[x1, x2, ..., xi],[y1, y2, ..., yi]]\n",
        "    \"\"\"\n",
        "    t = numpy.linspace(0, 2*numpy.pi, 1000)\n",
        "    x_noise, y_noise = numpy.random.rand(2, len(t))\n",
        "    \n",
        "    ellipse_x = center[0] + width*numpy.cos(t)*numpy.cos(phi)-height*numpy.sin(t)*numpy.sin(phi) + x_noise/2.\n",
        "    ellipse_y = center[1] + width*numpy.cos(t)*numpy.sin(phi)+height*numpy.sin(t)*numpy.cos(phi) + y_noise/2.\n",
        "\n",
        "    return [ellipse_x, ellipse_y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvYwYsinN6f-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#eyefitter.py\n",
        "\"\"\"\n",
        "Unless specified, all units are in pixels. \n",
        "All calculations are in camera frame (conversion would be commented)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class SingleEyeFitter(object):\n",
        "\n",
        "    def __init__(self, focal_length, pupil_radius, initial_eye_z, image_shape=(240, 320)):\n",
        "        self.focal_length = focal_length\n",
        "        self.image_shape = image_shape\n",
        "\n",
        "        self.pupil_radius = pupil_radius\n",
        "        self.vertex = [0, 0, -focal_length]\n",
        "        self.initial_eye_z = initial_eye_z\n",
        "\n",
        "        # (p,n) of unprojected gaze vector and pupil 3D position in SINGLE OBSERVATION\n",
        "        self.current_gaze_pos = 0  # reserved for (3,1) np.array in camera frame\n",
        "        self.current_gaze_neg = 0  # reserved for (3,1) np.array in camera frame\n",
        "        self.current_pupil_3Dcentre_pos = 0  # reserved for (3,1) np.array in camera frame\n",
        "        self.current_pupil_3Dcentre_neg = 0  # reserved for (3,1) np.array in camera frame\n",
        "        self.current_ellipse_centre = 0  # reserved for numpy array (2,1) in numpy indexing frame\n",
        "\n",
        "        # List of parameters across a number (m) of observations\n",
        "        self.unprojected_gaze_vectors = []  # A list: [\"gaze_positive\"~np(m,3), \"gaze_negative\"~np(m,3)]\n",
        "        self.unprojected_3D_pupil_positions = []  # [ \"pupil_3Dcentre_positive\"~np(m,3), \"pupil_3Dcentre_negative\"~np(m,3) ]\n",
        "        self.ellipse_centres = None  # reserved for numpy array (m,2) in numpy indexing frame,\n",
        "        # m = number of fitted ellipse centres corresponding to the projected gaze lines\n",
        "        self.selected_gazes = None  # reserved for (m,3) np.array in camera frame\n",
        "        self.selected_pupil_positions = None  # reserved for (m,3) np.array in camera frame\n",
        "\n",
        "        # Parameters of the eye model for consistent pupil estimate after initialisation\n",
        "        self.projected_eye_centre = None  # reserved for numpy array (2,1). Centre coordinate in numpy indexing frame.\n",
        "        self.eye_centre = None  # reserved for (3,1) numpy array. 3D centre coordinate in camera frame\n",
        "        self.aver_eye_radius = None  # Scaler\n",
        "\n",
        "        # Results of consistent pupil estimate\n",
        "        self.pupil_new_position_max = None  # numpy array (3,1)\n",
        "        self.pupil_new_position_min = None  # numpy array (3,1)\n",
        "        self.pupil_new_radius_max = None  # scalar\n",
        "        self.pupil_new_radius_min = None  # scalar\n",
        "        self.pupil_new_gaze_max = None  # numpy array (3,1)\n",
        "        self.pupil_new_gaze_min = None  # numpy array (3,1)\n",
        "\n",
        "    def unproject_single_observation(self, prediction, mask=None, threshold=0.5):\n",
        "        # \"prediction\" is an numpy array with shape (image_height, image_width) and data type: float [0-1]\n",
        "        # Our deeplearning model's outpupt is Y~(240, 320, 3),\n",
        "        # you will have to slice it manually as Y[:,:,1] as input to this function.\n",
        "        try:\n",
        "            assert len(prediction.shape) == 2\n",
        "            assert prediction.shape == self.image_shape\n",
        "        except(AssertionError):\n",
        "            raise AssertionError(\n",
        "                \"Shape of the observation input has to be (image_height, image_width) specified in the initialization of object, or if default, (240,320)\")\n",
        "        # Fit an ellipse from the prediction map\n",
        "        ellipse_info = fit_ellipse(prediction, mask=mask)\n",
        "        rr, cc, centre, w, h, radian = None, None, None, None, None, None\n",
        "        ellipse_confidence = 0\n",
        "\n",
        "        # We unproject the gaze vectors and pupil centre only if an ellipse has been detected\n",
        "        if ellipse_info is not None:\n",
        "\n",
        "            (rr, cc, centre, w, h, radian, ell) = ellipse_info\n",
        "            ellipse_confidence = computeEllipseConfidence(prediction, centre, w, h, radian)\n",
        "\n",
        "            # Convert centre coordinates from numpy indexing frame to camera frames\n",
        "            centre_cam = centre.copy()\n",
        "            centre_cam[0] = centre_cam[0] - self.image_shape[1] / 2\n",
        "            centre_cam[1] = centre_cam[1] - self.image_shape[0] / 2\n",
        "\n",
        "            # Convert ellipse parameters to the coefficients of the general form of ellipse equation\n",
        "            A, B, C, D, E, F = convert_ell_to_general(centre_cam[0], centre_cam[1], w, h, radian)\n",
        "            ell_co = (A, B, C, D, E, F)\n",
        "\n",
        "            # Unproject the ellipse to obtain 2 ambiguous gaze vectors with numpy shape (3,1),\n",
        "            # and pupil_centre with numpy shape (3,1)\n",
        "            unprojected_gaze_pos, unprojected_gaze_neg, unprojected_pupil_3Dcentre_pos, unprojected_pupil_3Dcentre_neg = unprojectGazePositions(\n",
        "                self.vertex, ell_co, self.pupil_radius)\n",
        "\n",
        "            # Normalize the gaze vectors and only take their real component\n",
        "            unprojected_gaze_pos = unprojected_gaze_pos / np.linalg.norm(unprojected_gaze_pos)\n",
        "            unprojected_gaze_neg = unprojected_gaze_neg / np.linalg.norm(unprojected_gaze_neg)\n",
        "\n",
        "            unprojected_gaze_pos, unprojected_gaze_neg, unprojected_pupil_3Dcentre_pos, unprojected_pupil_3Dcentre_neg = np.real(\n",
        "                unprojected_gaze_pos), np.real(unprojected_gaze_neg), np.real(unprojected_pupil_3Dcentre_pos), np.real(\n",
        "                unprojected_pupil_3Dcentre_neg)\n",
        "            self.current_gaze_pos, self.current_gaze_neg, self.current_pupil_3Dcentre_pos, self.current_pupil_3Dcentre_neg = unprojected_gaze_pos, unprojected_gaze_neg, unprojected_pupil_3Dcentre_pos, unprojected_pupil_3Dcentre_neg\n",
        "            self.current_ellipse_centre = np.array(centre).reshape(2, 1)\n",
        "        else:\n",
        "            self.current_gaze_pos, self.current_gaze_neg, self.current_pupil_3Dcentre_pos, self.current_pupil_3Dcentre_neg = None, None, None, None\n",
        "            self.current_ellipse_centre = None\n",
        "\n",
        "        return self.current_gaze_pos, self.current_gaze_neg, self.current_pupil_3Dcentre_pos, self.current_pupil_3Dcentre_neg, (\n",
        "        rr, cc, centre, w, h, radian, ellipse_confidence)\n",
        "\n",
        "    def add_to_fitting(self):\n",
        "        # Append parameterised gaze lines for fitting\n",
        "        if (self.current_gaze_pos is None) or (self.current_gaze_neg is None) or (\n",
        "                self.current_pupil_3Dcentre_pos is None) or (self.current_pupil_3Dcentre_neg is None) or (\n",
        "                self.current_ellipse_centre is None):\n",
        "            raise TypeError(\n",
        "                'No ellipse was caught in this observation, thus \"None\" is being added for fitting set, which is not allowed. Please manually skip this condition.')\n",
        "\n",
        "        # Store the gaze vectors and pupil 3D centres\n",
        "        if (len(self.unprojected_gaze_vectors) == 0) or (len(self.unprojected_3D_pupil_positions) == 0) or (\n",
        "                self.ellipse_centres is None):\n",
        "            self.unprojected_gaze_vectors.append(self.current_gaze_pos.reshape(1, 3))\n",
        "            self.unprojected_gaze_vectors.append(self.current_gaze_neg.reshape(1, 3))\n",
        "            self.unprojected_3D_pupil_positions.append(self.current_pupil_3Dcentre_pos.reshape(1, 3))\n",
        "            self.unprojected_3D_pupil_positions.append(self.current_pupil_3Dcentre_neg.reshape(1, 3))\n",
        "            self.ellipse_centres = self.current_ellipse_centre.reshape(1, 2)\n",
        "        else:\n",
        "            self.unprojected_gaze_vectors[0] = np.vstack(\n",
        "                (self.unprojected_gaze_vectors[0], self.current_gaze_pos.reshape(1, 3)))\n",
        "            self.unprojected_gaze_vectors[1] = np.vstack(\n",
        "                (self.unprojected_gaze_vectors[1], self.current_gaze_neg.reshape(1, 3)))\n",
        "            self.unprojected_3D_pupil_positions[0] = np.vstack(\n",
        "                (self.unprojected_3D_pupil_positions[0], self.current_pupil_3Dcentre_pos.reshape(1, 3)))\n",
        "            self.unprojected_3D_pupil_positions[1] = np.vstack(\n",
        "                (self.unprojected_3D_pupil_positions[1], self.current_pupil_3Dcentre_neg.reshape(1, 3)))\n",
        "            self.ellipse_centres = np.vstack((self.ellipse_centres, self.current_ellipse_centre.reshape(1, 2)))\n",
        "\n",
        "    def fit_projected_eye_centre(self, ransac=False, max_iters=1000, min_distance=2000):\n",
        "        # You will need to determine when to fit outside of the class\n",
        "        if (self.unprojected_gaze_vectors is None) or (self.ellipse_centres is None):\n",
        "            msg = \"No unprojected gaze lines or ellipse centres were found (not yet initalized). It is likely that the network fails to segment the pupil from the video. Please ensure your input video contains only a single eye but not other facial/body features.\"\n",
        "            raise TypeError(msg)\n",
        "\n",
        "        # Combining positive and negative gaze vectors\n",
        "        a = np.vstack((self.ellipse_centres, self.ellipse_centres))\n",
        "        n = np.vstack((self.unprojected_gaze_vectors[0][:, 0:2],\n",
        "                       self.unprojected_gaze_vectors[1][:, 0:2]))  # [:, 0:2] takes only 2D projection\n",
        "\n",
        "        # Normalisation of the 2D projection of gaze vectors is done inside intersect()\n",
        "        if ransac == True:\n",
        "            samples_to_fit = np.ceil(a.shape[0]/5).astype(np.int)  # Assuming 20% of outliners\n",
        "            self.projected_eye_centre = fit_ransac(a, n, max_iters=max_iters, samples_to_fit=samples_to_fit,\n",
        "                                                   min_distance=min_distance)\n",
        "        else:\n",
        "            self.projected_eye_centre = intersect(a, n)\n",
        "        if (self.projected_eye_centre is None):\n",
        "            raise TypeError(\"Projected_eye_centre was not fitted. You may need -v and -m argument to check whether the pupil segmentation works properly.\")\n",
        "        return self.projected_eye_centre\n",
        "\n",
        "    def estimate_eye_sphere(self):\n",
        "        # This function is called once after fit_projected_eye_centre()\n",
        "        # self.initial_eye_z is required (in pixel unit)\n",
        "        # self.initial_eye_z shall be the z-distance between the point and camera vertex (in camera frame)\n",
        "        if (self.projected_eye_centre is None):\n",
        "            # pdb.set_trace()\n",
        "            raise TypeError('Projected_eye_centre must be initialized first')\n",
        "\n",
        "        # Unprojecting the 2D projected eye centre to 3D.\n",
        "        # Converting the projected_eye_centre from numpy indexing frame to camera frame\n",
        "        projected_eye_centre_camera_frame = self.projected_eye_centre.copy()\n",
        "        projected_eye_centre_camera_frame[0] = projected_eye_centre_camera_frame[0] - self.image_shape[1] / 2\n",
        "        projected_eye_centre_camera_frame[1] = projected_eye_centre_camera_frame[1] - self.image_shape[0] / 2\n",
        "\n",
        "        # Unprojection: Nearest intersection of two lines. \n",
        "        # a = [eye_centre, pupil_3Dcentre], n =[gaze_vector, pupil_3D_centre]\n",
        "        projected_eye_centre_camera_frame_scaled = reverse_reproject(projected_eye_centre_camera_frame,\n",
        "                                                                     self.initial_eye_z, self.focal_length)\n",
        "        eye_centre_camera_frame = np.append(projected_eye_centre_camera_frame_scaled, self.initial_eye_z).reshape(3, 1)\n",
        "\n",
        "        # Reconstructed selected gaze vectors and pupil positions by rejecting those pointing away from projected eyecentre\n",
        "        m = self.unprojected_gaze_vectors[0].shape[0]\n",
        "        for i in range(m):\n",
        "            gazes = [self.unprojected_gaze_vectors[0][i, :].reshape(3, 1),\n",
        "                     self.unprojected_gaze_vectors[1][i, :].reshape(3, 1)]\n",
        "            positions = [self.unprojected_3D_pupil_positions[0][i, :].reshape(3, 1),\n",
        "                         self.unprojected_3D_pupil_positions[1][i, :].reshape(3, 1)]\n",
        "            selected_gaze, selected_position = self.select_pupil_from_single_observation(gazes, positions,\n",
        "                                                                                         eye_centre_camera_frame)\n",
        "\n",
        "            self.selected_gazes, self.selected_pupil_positions = self.stacking_from_nx1_to_mxn(\n",
        "                [self.selected_gazes, self.selected_pupil_positions],\n",
        "                [selected_gaze, selected_position],\n",
        "                [3, 3])\n",
        "\n",
        "        radius_counter = []\n",
        "        for i in range(self.selected_gazes.shape[0]):\n",
        "            gaze = self.selected_gazes[i, :].reshape(1, 3)\n",
        "            position = self.selected_pupil_positions[i, :].reshape(1, 3)\n",
        "\n",
        "            # Before stacking, you must reshape (3,1) to (1,3)\n",
        "            a_3Dfitting = np.vstack((eye_centre_camera_frame.reshape(1, 3), position))\n",
        "            n_3Dfitting = np.vstack((gaze, (position / np.linalg.norm(position))))\n",
        "\n",
        "            intersected_pupil_3D_centre = intersect(a_3Dfitting, n_3Dfitting)\n",
        "            radius = np.linalg.norm(intersected_pupil_3D_centre - eye_centre_camera_frame)\n",
        "            radius_counter.append(radius)\n",
        "        aver_radius = np.mean(radius_counter)\n",
        "\n",
        "        self.aver_eye_radius = aver_radius\n",
        "        self.eye_centre = eye_centre_camera_frame\n",
        "        return aver_radius, radius_counter\n",
        "\n",
        "    def gen_consistent_pupil(self):\n",
        "        # This function must be called after using unproject_single_observation() to update surrent observation\n",
        "        if (self.eye_centre is None) or (self.aver_eye_radius is None):\n",
        "            raise TypeError(\"Call estimate_eye_sphere() to initialize eye_centre and eye_radius first.\")\n",
        "        else:\n",
        "            selected_gaze, selected_position = self.select_pupil_from_single_observation(\n",
        "                [self.current_gaze_pos, self.current_gaze_neg],\n",
        "                [self.current_pupil_3Dcentre_pos, self.current_pupil_3Dcentre_neg], self.eye_centre)\n",
        "            o = np.zeros((3, 1))\n",
        "\n",
        "            try:\n",
        "                d1, d2 = line_sphere_intersect(self.eye_centre, self.aver_eye_radius, o,\n",
        "                                               selected_position / np.linalg.norm(selected_position))\n",
        "                new_position_min = o + min([d1, d2]) * (selected_position / np.linalg.norm(selected_position))\n",
        "                new_position_max = o + max([d1, d2]) * (selected_position / np.linalg.norm(selected_position))\n",
        "                new_radius_min = (self.pupil_radius / selected_position[2, 0]) * new_position_min[2, 0]\n",
        "                new_radius_max = (self.pupil_radius / selected_position[2, 0]) * new_position_max[2, 0]\n",
        "\n",
        "                new_gaze_min = new_position_min - self.eye_centre\n",
        "                new_gaze_min = new_gaze_min / np.linalg.norm(new_gaze_min)\n",
        "\n",
        "                new_gaze_max = new_position_max - self.eye_centre\n",
        "                new_gaze_max = new_gaze_max / np.linalg.norm(new_gaze_max)\n",
        "                self.pupil_new_position_min, self.pupil_new_position_max = new_position_min, new_position_max\n",
        "                self.pupil_new_radius_min, self.pupil_new_radius_max = new_radius_min, new_radius_max\n",
        "                self.pupil_new_gaze_min, self.pupil_new_gaze_max = new_gaze_min, new_gaze_max\n",
        "                consistence = True\n",
        "\n",
        "            except(NoIntersectionError):\n",
        "                # print(\"Cannot find line-sphere interception. Old pupil parameters are used.\")\n",
        "                new_position_min, new_position_max = selected_position, selected_position\n",
        "                new_gaze_min, new_gaze_max = selected_gaze, selected_gaze\n",
        "                new_radius_min, new_radius_max = self.pupil_radius, self.pupil_radius\n",
        "                consistence = False\n",
        "\n",
        "            return [new_position_min, new_position_max], [new_gaze_min, new_gaze_max], [new_radius_min,\n",
        "                                                                                        new_radius_max], consistence\n",
        "\n",
        "    def plot_gaze_lines(self, ax):\n",
        "        t = np.linspace(-1000, 1000, 1000)\n",
        "        a = np.vstack((self.ellipse_centres, self.ellipse_centres))\n",
        "        n = np.vstack((self.unprojected_gaze_vectors[0][:, 0:2],\n",
        "                       self.unprojected_gaze_vectors[1][:, 0:2]))  # [:, 0:2] takes only 2D projection\n",
        "\n",
        "        for i in range(a.shape[0]):\n",
        "            a_each = a[i, :]\n",
        "            n_each = n[i, :]\n",
        "\n",
        "            points = np.array(a_each).reshape(2, 1) + (t * n_each[0:2].reshape(2, 1))\n",
        "            ax.plot(points[0, :], points[1, :])\n",
        "        ax.set_xlim(0, self.image_shape[1])\n",
        "        ax.set_ylim(self.image_shape[0], 0)\n",
        "        return ax\n",
        "\n",
        "    def select_pupil_from_single_observation(self, gazes, positions, eye_centre_camera_frame):\n",
        "        # gazes is a list ~ [gaze_vector_pos~(3,1), gaze_vector_neg~(3,1)]\n",
        "        # positions is a list ~ [pupil_position_pos~(3,1), pupil_position_neg~(3,1)]\n",
        "        # eye_centre_camera_frame ~ numpy array~(3,1)\n",
        "\n",
        "        selected_gaze = gazes[0]\n",
        "        selected_position = positions[0]\n",
        "        projected_centre = reproject(eye_centre_camera_frame, self.focal_length)\n",
        "        projected_gaze = reproject(selected_position + selected_gaze, self.focal_length) - projected_centre\n",
        "        projected_position = reproject(selected_position, self.focal_length)\n",
        "        if np.dot(projected_gaze.T, (projected_position - projected_centre)) > 0:\n",
        "            return selected_gaze, selected_position\n",
        "        else:\n",
        "            return gazes[1], positions[1]\n",
        "\n",
        "    @staticmethod\n",
        "    def stacking_from_nx1_to_mxn(stacked_arrays_list, stacked_vectors_list, dims_list):\n",
        "        list_as_array = np.array([stacked_arrays_list])\n",
        "        new_stacked_arrays_list = []\n",
        "        if np.all(list_as_array == None):\n",
        "            for stacked_array, stacked_vector, n in zip(stacked_arrays_list, stacked_vectors_list, dims_list):\n",
        "                stacked_array = stacked_vector.reshape(1, n)\n",
        "                new_stacked_arrays_list.append(stacked_array)\n",
        "        elif np.all(list_as_array != None):\n",
        "            for stacked_array, stacked_vector, n in zip(stacked_arrays_list, stacked_vectors_list, dims_list):\n",
        "                stacked_array = np.vstack((stacked_array, stacked_vector.reshape(1, n)))\n",
        "                new_stacked_arrays_list.append(stacked_array)\n",
        "        elif np.any(list_as_array == None):\n",
        "            print(\"Error list =\\n\", stacked_arrays_list)\n",
        "            raise TypeError(\"Some lists are initialized, some are not ('None'). Error has happened!\")\n",
        "        else:\n",
        "            print(\"Error list =\\n\", stacked_arrays_list)\n",
        "            raise TypeError(\"Unknown Error Occurred.\")\n",
        "        return new_stacked_arrays_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH5tS9DVOmlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#inferer.py\n",
        "class gaze_inferer(object):\n",
        "    def __init__(self, model, flen, ori_video_shape, sensor_size, infer_gaze_flag=True):\n",
        "        \"\"\"\n",
        "        Initialize necessary parameters and load deep_learning model\n",
        "        \n",
        "        Args:\n",
        "            model: Deep learning model that perform image segmentation. Pre-trained model is provided at https://github.com/pydsgz/DeepVOG/model/DeepVOG_model.py, simply by loading load_DeepVOG() with \"DeepVOG_weights.h5\" in the same directory. If you use your own model, it should take input of grayscale image (m, 240, 320, 3) with value float [0,1] and output (m, 240, 320, 3) with value float [0,1] where (m, 240, 320, 1) is the pupil map.\n",
        "            \n",
        "            flen (float): Focal length of camera in mm. You can look it up at the product menu of your camera\n",
        "            \n",
        "            ori_video_shape (tuple or list or np.ndarray): Original video shape from your camera, (height, width) in pixel. If you cropped the video before, use the \"original\" shape but not the cropped shape\n",
        "            \n",
        "            sensor_size (tuple or list or np.ndarray): Sensor size of your camera, (height, width) in mm. For 1/3 inch CMOS sensor, it should be (3.6, 4.8). Further reference can be found in https://en.wikipedia.org/wiki/Image_sensor_format and you can look up in your camera product menu\n",
        "        \n",
        "        \"\"\"\n",
        "        # Assertion of shape\n",
        "        try:\n",
        "            assert ((isinstance(flen, int) or isinstance(flen, float)))\n",
        "            assert (isinstance(ori_video_shape, tuple) or isinstance(ori_video_shape, list) or isinstance(\n",
        "                ori_video_shape, np.ndarray))\n",
        "            assert (isinstance(sensor_size, tuple) or isinstance(sensor_size, list) or isinstance(sensor_size,\n",
        "                                                                                                  np.ndarray))\n",
        "            assert (isinstance(sensor_size, tuple) or isinstance(sensor_size, list) or isinstance(sensor_size,\n",
        "                                                                                                  np.ndarray))\n",
        "        except AssertionError:\n",
        "            print(\"At least one of your arguments does not have correct type\")\n",
        "            raise TypeError\n",
        "\n",
        "        # Parameters dealing with camera and video shape\n",
        "        self.flen = flen\n",
        "        self.ori_video_shape, self.sensor_size = np.array(ori_video_shape).squeeze(), np.array(sensor_size).squeeze()\n",
        "        self.mm2px_scaling = np.linalg.norm(self.ori_video_shape) / np.linalg.norm(self.sensor_size)\n",
        "\n",
        "        self.model = model\n",
        "        self.confidence_fitting_threshold = 0.96\n",
        "        self.eyefitter = SingleEyeFitter(focal_length=self.flen * self.mm2px_scaling,\n",
        "                                         pupil_radius=2 * self.mm2px_scaling,\n",
        "                                         initial_eye_z=50 * self.mm2px_scaling)\n",
        "        self.infer_gaze_flag = infer_gaze_flag\n",
        "    def process(self, video_src, mode, output_record_path=\"\", batch_size=32,\n",
        "                output_video_path=\"\", heatmap=False, print_prefix=\"\"):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        video_src : str\n",
        "            Path of the video from which you want to (1) fit the eyeball model or (2) infer the gaze.\n",
        "        mode : str\n",
        "            There are two modes: \"Fit\" or \"Infer\". \"Fit\" will fit an eyeball model from the video source.\n",
        "            \"Infer\" will infer the gaze from the video source.\n",
        "        batch_size : int\n",
        "            Batch size. Recommended >= 32.\n",
        "        output_record_path : str\n",
        "            Path of the csv file of your gaze estimation result. Only matter if the mode == \"Infer\".\n",
        "        output_video_path : str\n",
        "            Path of the output visualization video. If mode == \"Fit\", it draws segmented pupil ellipse.\n",
        "            If mode == \"Infer\", it draws segmented pupil ellipse and gaze vector. if output_video_path == \"\",\n",
        "            no visualization will be produced.\n",
        "        heatmap : bool\n",
        "            If True, show heatmap in the visualization video. If False, no heatmap will be shown.\n",
        "        print_prefix : str\n",
        "            What to print before the progress text.\n",
        "        Returns\n",
        "        -------\n",
        "        None\n",
        "        \"\"\"\n",
        "        # Get video information (path strings, frame reader, video's shapes...etc)\n",
        "        video_name_root, ext, vreader, vid_shapes, shape_correct, image_scaling_factor = self._get_video_info(video_src)\n",
        "        (vid_m, vid_w, vid_h, vid_channels) = vid_shapes\n",
        "\n",
        "        self.vid_manager = VideoManager(vreader=vreader, output_record_path=output_record_path,\n",
        "                                        output_video_path=output_video_path, heatmap=heatmap)\n",
        "\n",
        "        # (predict) Check if the eyeball model is imported\n",
        "        if mode == \"Infer\":\n",
        "            self._check_eyeball_model_exists()\n",
        "\n",
        "        # Correct eyefitter's parameters in accordance with the image resizing\n",
        "        self.eyefitter.focal_length = self.flen * self.mm2px_scaling * image_scaling_factor\n",
        "        self.eyefitter.pupil_radius = 2 * self.mm2px_scaling * image_scaling_factor\n",
        "\n",
        "        # Set batch-wise operation details\n",
        "        initial_frame, final_frame = 0, vid_m\n",
        "        final_batch_size = vid_m % batch_size\n",
        "        final_batch_idx = vid_m - final_batch_size\n",
        "        X_batch = np.zeros((batch_size, 240, 320, 3))\n",
        "        X_batch_final = np.zeros((vid_m % batch_size, 240, 320, 3))\n",
        "\n",
        "        # Start looping for batch-wise processing\n",
        "        for idx, frame in enumerate(vreader.nextFrame()):\n",
        "\n",
        "            print(\"\\r%s%s %s (%d/%d)\" % (print_prefix, mode, video_name_root + ext, idx + 1, vid_m), end=\"\", flush=True)\n",
        "\n",
        "            frame_preprocessed = self._preprocess_image(frame, shape_correct)\n",
        "            mini_batch_idx = idx % batch_size\n",
        "\n",
        "            # Before reaching the batch size, stack the array\n",
        "            if ((mini_batch_idx != 0) and (idx < final_batch_idx)) or (idx == 0):\n",
        "                X_batch[mini_batch_idx, :, :, :] = frame_preprocessed\n",
        "\n",
        "            # After reaching the batch size, but not the final batch, predict heatmap and fit/infer angles\n",
        "            elif (mini_batch_idx == 0) and (idx < final_batch_idx) or (idx == final_batch_idx):\n",
        "                Y_batch = self.model.predict(X_batch)\n",
        "                if mode == \"Fit\":\n",
        "                    self._fitting_batch(X_batch=X_batch,\n",
        "                                        Y_batch=Y_batch)\n",
        "                elif mode == \"Infer\":\n",
        "                    _, _, _ = self._infer_batch(X_batch=X_batch,\n",
        "                                                Y_batch=Y_batch,\n",
        "                                                idx=idx - final_batch_size)\n",
        "\n",
        "                # Renew X_batch for next batch\n",
        "                X_batch = np.zeros((batch_size, 240, 320, 3))\n",
        "                X_batch[mini_batch_idx, :, :, :] = frame_preprocessed\n",
        "\n",
        "            # Within the final batch but not yet reaching the last index, stack the array\n",
        "            elif (idx > final_batch_idx) and (idx != final_frame - 1):\n",
        "                X_batch_final[idx - final_batch_idx, :, :, :] = frame_preprocessed\n",
        "\n",
        "            # Within the final batch and reaching the last index, predict heatmap and fit/infer angles\n",
        "            elif idx == final_frame - 1:\n",
        "                X_batch_final[idx - final_batch_idx, :, :, :] = frame_preprocessed\n",
        "                Y_batch = self.model.predict(X_batch_final)\n",
        "                if mode == \"Fit\":\n",
        "                    self._fitting_batch(X_batch=X_batch_final,\n",
        "                                        Y_batch=Y_batch)\n",
        "                elif mode == \"Infer\":\n",
        "                    _, _, _ = self._infer_batch(X_batch=X_batch_final,\n",
        "                                                Y_batch=Y_batch,\n",
        "                                                idx=idx - final_batch_size)\n",
        "\n",
        "        if mode == \"Fit\":\n",
        "            # Fit eyeball models. Parameters are stored as internal attributes of Eyefitter instance.\n",
        "            _ = self.eyefitter.fit_projected_eye_centre(ransac=True, max_iters=100, min_distance=10*vid_m)\n",
        "            _, _ = self.eyefitter.estimate_eye_sphere()\n",
        "\n",
        "            # Issue error if eyeball model still does not exist after fitting.\n",
        "            if (self.eyefitter.eye_centre is None) or (self.eyefitter.aver_eye_radius is None):\n",
        "                raise TypeError(\"Eyeball model was not fitted. You may need -v or -m argument to check whether the pupil segmentation works properly.\")\n",
        "        print()\n",
        "\n",
        "    def save_eyeball_model(self, path):\n",
        "        \"\"\"\n",
        "        Save eyeball model parameters in json format.\n",
        "        \n",
        "        Args:\n",
        "            path (str): path of the eyeball model file.\n",
        "        \"\"\"\n",
        "\n",
        "        if (self.eyefitter.eye_centre is None) or (self.eyefitter.aver_eye_radius is None):\n",
        "            print(\"3D eyeball model not found. You may need -v or -m argument to check whether the pupil segmentation works properly.\")\n",
        "            raise\n",
        "        else:\n",
        "            save_dict = {\"eye_centre\": self.eyefitter.eye_centre.tolist(),\n",
        "                         \"aver_eye_radius\": self.eyefitter.aver_eye_radius}\n",
        "            save_json(path, save_dict)\n",
        "\n",
        "    def load_eyeball_model(self, path):\n",
        "        \"\"\"\n",
        "        Load eyeball model parameters of json format from path.\n",
        "        \n",
        "        Args:\n",
        "            path (str): path of the eyeball model file.\n",
        "        \"\"\"\n",
        "        loaded_dict = load_json(path)\n",
        "        if (self.eyefitter.eye_centre is not None) or (self.eyefitter.aver_eye_radius is not None):\n",
        "            warnings.warn(\"3D eyeball exists and reloaded\")\n",
        "\n",
        "        self.eyefitter.eye_centre = np.array(loaded_dict[\"eye_centre\"])\n",
        "        self.eyefitter.aver_eye_radius = loaded_dict[\"aver_eye_radius\"]\n",
        "\n",
        "\n",
        "    def _fitting_batch(self, X_batch, Y_batch):\n",
        "\n",
        "        if self.vid_manager.output_video_flag:\n",
        "            # Convert video frames to 8 bit integer format for drawing the output video frames\n",
        "            video_frames_batch = np.around(X_batch * 255).astype(int)\n",
        "            vid_frame_shape_2d = (video_frames_batch.shape[1], video_frames_batch.shape[2])\n",
        "\n",
        "        for batch_idx, (X_each, Y_each) in enumerate(zip(X_batch, Y_batch)):\n",
        "            pred_each = Y_each[:, :, 1]\n",
        "            _, _, _, _, ellipse_info = self.eyefitter.unproject_single_observation(pred_each)\n",
        "            (rr, cc, centre, w, h, radian, ellipse_confidence) = ellipse_info\n",
        "\n",
        "            # If visualization is true, initialize output frame\n",
        "            if self.vid_manager.output_video_flag:\n",
        "                vid_frame = video_frames_batch[batch_idx,]\n",
        "\n",
        "            # Fit each observation to eyeball model\n",
        "            if centre is not None:\n",
        "                if (ellipse_confidence > self.confidence_fitting_threshold):\n",
        "                    self.eyefitter.add_to_fitting()\n",
        "\n",
        "                # Draw ellipse and pupil centre on input video if visualization is enabled\n",
        "                if self.vid_manager.output_video_flag:\n",
        "                    ellipse_centre_np = np.array(centre)\n",
        "\n",
        "                    # Draw pupil ellipse\n",
        "                    vid_frame = draw_ellipse(output_frame=vid_frame, frame_shape=vid_frame_shape_2d,\n",
        "                                             ellipse_info=ellipse_info, color=[255, 255, 0])\n",
        "                    # Draw small circle at the ellipse centre\n",
        "                    vid_frame = draw_circle(output_frame=vid_frame, frame_shape=vid_frame_shape_2d,\n",
        "                                            centre=ellipse_centre_np, radius=5, color=[0, 255, 0])\n",
        "\n",
        "                    self.vid_manager.write_frame_with_condition(vid_frame=vid_frame, pred_each=pred_each)\n",
        "            else:\n",
        "                # Draw original input frame when no ellipse is found\n",
        "                if self.vid_manager.output_video_flag:\n",
        "                    self.vid_manager.write_frame_with_condition(vid_frame=vid_frame, pred_each=pred_each)\n",
        "\n",
        "    def _infer_batch(self, X_batch, Y_batch, idx):\n",
        "\n",
        "        if self.vid_manager.output_video_flag:\n",
        "            # Convert video frames to 8 bit integer format for drawing the output video frames\n",
        "            video_frames_batch = np.around(X_batch * 255).astype(int)\n",
        "            vid_frame_shape_2d = (video_frames_batch.shape[1], video_frames_batch.shape[2])\n",
        "\n",
        "        for batch_idx, (X_each, Y_each) in enumerate(zip(X_batch, Y_batch)):\n",
        "\n",
        "            frame = idx + batch_idx + 1\n",
        "            pred_each = Y_each[:, :, 1]\n",
        "            _, _, _, _, ellipse_info = self.eyefitter.unproject_single_observation(pred_each)\n",
        "            (rr, cc, centre, w, h, radian, ellipse_confidence) = ellipse_info\n",
        "            # If visualization is true, initialize output frame for drawing\n",
        "            if self.vid_manager.output_video_flag:\n",
        "                vid_frame = video_frames_batch[batch_idx,]\n",
        "\n",
        "            # If ellipse fitting is successful, i.e. an ellipse is located, AND gaze inference is ENABLED\n",
        "            if (centre is not None) and self.infer_gaze_flag:\n",
        "                p_list, n_list, _, consistence = self.eyefitter.gen_consistent_pupil()\n",
        "                p1, n1 = p_list[0], n_list[0]\n",
        "                px, py, pz = p1[0, 0], p1[1, 0], p1[2, 0]\n",
        "                x, y = convert_vec2angle31(n1)\n",
        "                positions = (px, py, pz, centre[0], centre[1])  # Pupil 3D positions and 2D projected positions\n",
        "                gaze_angles = (x, y)  # horizontal and vertical gaze angles\n",
        "                inference_confidence = (ellipse_confidence, consistence)\n",
        "                self.vid_manager.write_results(frame_id=frame, pupil2D_x=centre[0], pupil2D_y=centre[1], gaze_x=x,\n",
        "                                               gaze_y=y, confidence=ellipse_confidence, consistence=consistence)\n",
        "\n",
        "                if self.vid_manager.output_video_flag:\n",
        "                    # # Code below is for drawing video\n",
        "                    ellipse_centre_np = np.array(centre)\n",
        "                    projected_eye_centre = reproject(self.eyefitter.eye_centre,\n",
        "                                                     self.eyefitter.focal_length)  # shape (2,1)\n",
        "                    # The lines below are for translation from camera coordinate system (centred at image centre)\n",
        "                    # to numpy's indexing frame. You substract the vector by the half of the video's 2D shape.\n",
        "                    # Col = x-axis, Row = y-axis\n",
        "                    projected_eye_centre += np.array(vid_frame_shape_2d).T.reshape(-1, 1) / 2\n",
        "\n",
        "                    vid_frame = self._draw_vis_on_frame(vid_frame, vid_frame_shape_2d, ellipse_info, ellipse_centre_np,\n",
        "                                                        projected_eye_centre, gaze_vec=n1)\n",
        "\n",
        "                    self.vid_manager.write_frame_with_condition(vid_frame=vid_frame, pred_each=pred_each)\n",
        "\n",
        "            # If ellipse fitting is successful, i.e. an ellipse is located, AND gaze inference is DISABLED\n",
        "            elif (centre is not None) and (not self.infer_gaze_flag):\n",
        "                positions, gaze_angles, inference_confidence = None, None, None\n",
        "                self.vid_manager.write_results(frame_id=frame, pupil2D_x=centre[0], pupil2D_y=centre[1], gaze_x=np.nan,\n",
        "                                               gaze_y=np.nan, confidence=ellipse_confidence, consistence=np.nan)\n",
        "                if self.vid_manager.output_video_flag:\n",
        "                    ellipse_centre_np = np.array(centre)\n",
        "\n",
        "                    # Draw pupil ellipse\n",
        "                    vid_frame = draw_ellipse(output_frame=vid_frame, frame_shape=vid_frame_shape_2d,\n",
        "                                             ellipse_info=ellipse_info, color=[255, 255, 0])\n",
        "                    # Draw small circle at the ellipse centre\n",
        "                    vid_frame = draw_circle(output_frame=vid_frame, frame_shape=vid_frame_shape_2d,\n",
        "                                            centre=ellipse_centre_np, radius=5, color=[0, 255, 0])\n",
        "                    self.vid_manager.write_frame_with_condition(vid_frame=vid_frame, pred_each=pred_each)\n",
        "\n",
        "            # IF ellipse fitting is unsuccessful.\n",
        "            else:\n",
        "                # If ellipse cannot be found, fill the outputs with None's\n",
        "                positions, gaze_angles, inference_confidence = None, None, None\n",
        "\n",
        "                self.vid_manager.write_results(frame_id=frame, pupil2D_x=np.nan, pupil2D_y=np.nan, gaze_x=np.nan,\n",
        "                                               gaze_y=np.nan, confidence=np.nan, consistence=np.nan)\n",
        "\n",
        "                # Draw original input frame when no ellipse is found\n",
        "                if self.vid_manager.output_video_flag:\n",
        "                    self.vid_manager.write_frame_with_condition(vid_frame=vid_frame, pred_each=pred_each)\n",
        "\n",
        "        return positions, gaze_angles, inference_confidence\n",
        "\n",
        "    def _get_video_info(self, video_src):\n",
        "        video_name_with_ext = os.path.split(video_src)[1]\n",
        "        video_name_root, ext = os.path.splitext(video_name_with_ext)\n",
        "        vreader = skv.FFmpegReader(video_src)\n",
        "        m, w, h, channels = vreader.getShape()\n",
        "        image_scaling_factor = np.linalg.norm((240, 320)) / np.linalg.norm((h, w))\n",
        "        shape_correct = self._inspectVideoShape(w, h)\n",
        "        return video_name_root, ext, vreader, (m, w, h, channels), shape_correct, image_scaling_factor\n",
        "\n",
        "    def _check_eyeball_model_exists(self):\n",
        "        try:\n",
        "            if self.infer_gaze_flag:\n",
        "                assert isinstance(self.eyefitter.eye_centre, np.ndarray)\n",
        "                assert self.eyefitter.eye_centre.shape == (3, 1)\n",
        "                assert self.eyefitter.aver_eye_radius is not None\n",
        "            else:\n",
        "                pass\n",
        "        except AssertionError as e:\n",
        "            print(\n",
        "                \"3D eyeball mode is not found. Gaze inference cannot continue. Please fit/load an eyeball model first\")\n",
        "            raise e\n",
        "\n",
        "    @staticmethod\n",
        "    def _inspectVideoShape(w, h):\n",
        "        if (w, h) == (240, 320):\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def _computeCroppedShape(ori_video_shape, crop_size):\n",
        "        video = np.zeros(ori_video_shape)\n",
        "        cropped = video[crop_size[0]:crop_size[1], crop_size[2], crop_size[3]]\n",
        "        return cropped.shape\n",
        "\n",
        "    @staticmethod\n",
        "    def _preprocess_image(img, shape_correct):\n",
        "        \"\"\"\n",
        "        \n",
        "        Args:\n",
        "            img (numpy array): unprocessed image with shape (w, h, 3) and values int [0, 255]\n",
        "        Returns:\n",
        "            output_img (numpy array): processed grayscale image with shape ( 240, 320, 1) and values float [0,1]\n",
        "        \"\"\"\n",
        "        output_img = np.zeros((240, 320, 3))\n",
        "        img = img / 255\n",
        "        img = rgb2gray(img)\n",
        "        if not shape_correct:\n",
        "            img = resize(img, (240, 320))\n",
        "        output_img[:, :, :] = img.reshape(240, 320, 1)\n",
        "        return output_img\n",
        "\n",
        "    @staticmethod\n",
        "    def _draw_vis_on_frame(vid_frame, vid_frame_shape_2d, ellipse_info, ellipse_centre_np, projected_eye_centre,\n",
        "                           gaze_vec):\n",
        "\n",
        "        # Draw pupil ellipse\n",
        "        vid_frame = draw_ellipse(output_frame=vid_frame, frame_shape=vid_frame_shape_2d,\n",
        "                                 ellipse_info=ellipse_info, color=[255, 255, 0])\n",
        "\n",
        "        # Draw from eyeball centre to ellipse centre (just connecting two points)\n",
        "        vec_with_length = ellipse_centre_np - projected_eye_centre.squeeze()\n",
        "        vid_frame = draw_line(output_frame=vid_frame, frame_shape=vid_frame_shape_2d, o=projected_eye_centre,\n",
        "                              l=vec_with_length, color=[0, 0, 255])\n",
        "\n",
        "        # Draw gaze vector originated from ellipse centre\n",
        "        vid_frame = draw_line(output_frame=vid_frame, frame_shape=vid_frame_shape_2d, o=ellipse_centre_np,\n",
        "                              l=gaze_vec * 50, color=[255, 0, 0])\n",
        "\n",
        "        # Draw small circle at the ellipse centre\n",
        "        vid_frame = draw_circle(output_frame=vid_frame, frame_shape=vid_frame_shape_2d,\n",
        "                                centre=ellipse_centre_np, radius=5, color=[0, 255, 0])\n",
        "        return vid_frame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_7S6hzFPBu3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#intersection.py\n",
        "class NoIntersectionError(Exception):\n",
        "    pass\n",
        "\n",
        "# vector (m,n) , m = number of examples, n = dimensionality\n",
        "# a = coordinates of the vector\n",
        "# n = orientation of the vector\n",
        "def intersect(a, n):\n",
        "    # default normalisation of vectors n\n",
        "    n = n/np.linalg.norm(n, axis = 1, keepdims=True)\n",
        "    num_lines = a.shape[0]\n",
        "    dim = a.shape[1]\n",
        "    I = np.eye(dim)\n",
        "    R_sum = 0\n",
        "    q_sum = 0\n",
        "    for i in range(num_lines):    \n",
        "        R = I - np.matmul(n[i].reshape(dim,1), n[i].reshape(1,dim))\n",
        "\n",
        "        q = np.matmul(R,a[i].reshape(dim,1))\n",
        "        q_sum = q_sum + q\n",
        "        R_sum = R_sum + R\n",
        "    p = np.matmul(np.linalg.inv(R_sum),q_sum)\n",
        "    return p\n",
        "\n",
        "def calc_distance(a,n, p):\n",
        "    num_lines = a.shape[0]\n",
        "    dim = a.shape[1]\n",
        "    I = np.eye(dim)\n",
        "    D_sum = 0\n",
        "    for i in range(num_lines):\n",
        "        D_1 = (a[i].reshape(dim,1) - p.reshape(dim,1)).T\n",
        "        D_2 = I - np.matmul(n[i].reshape(dim,1), n[i].reshape(1,dim))\n",
        "        D_3 = D_1.T\n",
        "        D = np.matmul(np.matmul(D_1,D_2),D_3)\n",
        "        D_sum = D_sum + D\n",
        "    D_sum = D_sum/num_lines\n",
        "    return D_sum\n",
        "\n",
        "def fit_ransac(a,n, max_iters = 2000, samples_to_fit = 20, min_distance = 2000):\n",
        "    num_lines = a.shape[0]\n",
        "    \n",
        "    best_model = None\n",
        "    best_distance = min_distance\n",
        "    for i in range(max_iters):\n",
        "        # print(\"\\rRANSAC: Currently {0}\".format(i), flush=True)\n",
        "        sampling_index = np.random.choice(num_lines, size = samples_to_fit, replace=False)\n",
        "        a_sampled = a[sampling_index,:]\n",
        "        n_sampled = n[sampling_index,:]\n",
        "        model_sampled = intersect(a_sampled, n_sampled)\n",
        "        sampled_distance = calc_distance( a,n, model_sampled)\n",
        "        # print(sampled_distance)\n",
        "        if sampled_distance > min_distance:\n",
        "            continue\n",
        "        else:\n",
        "            if sampled_distance < best_distance:\n",
        "                best_model = model_sampled\n",
        "                best_distance = sampled_distance\n",
        "    # if best_model is None:\n",
        "    #     best_model = model_sampled\n",
        "    return best_model\n",
        "\n",
        "def line_sphere_intersect(c, r, o, l):\n",
        "    # c = numpy array (3,1). Centre of the eyeball\n",
        "    # r = scaler. Radius of the eyeball\n",
        "    # o = numpy array (3,1). Origin of the line\n",
        "    # l = numpy array (3,1). Directional unit vector of the line\n",
        "    # return [d1, d2] : auxilary variables of the parametrised line x = o + dl\n",
        "    # the closer one to the camera is chosen\n",
        "    l = l/np.linalg.norm(l)\n",
        "    delta = np.square(np.dot(l.T,(o-c))) - np.dot((o-c).T,(o-c)) + np.square(r)\n",
        "    if delta < 0:\n",
        "        raise NoIntersectionError\n",
        "    else:\n",
        "        d1 = -np.dot(l.T,(o-c)) + np.sqrt(delta)\n",
        "        d2 = -np.dot(l.T,(o-c)) - np.sqrt(delta)\n",
        "    return [d1,d2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yX_IchFPY0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#jobman.py\n",
        "class deepvog_jobman_CLI(object):\n",
        "    def __init__(self, gpu_num, flen, ori_video_shape, sensor_size, batch_size):\n",
        "        \"\"\"\n",
        "        \n",
        "        Args:\n",
        "            gpu_num (str)\n",
        "            flen (float)\n",
        "            ori_video_shape (tuple)\n",
        "            sensor_size (tuple)\n",
        "            batch_size (int)\n",
        "        \n",
        "        \"\"\"\n",
        "        os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_num\n",
        "        self.model = load_DeepVOG()\n",
        "        self.flen = float(flen)\n",
        "        self.ori_video_shape = ori_video_shape\n",
        "        self.sensor_size = sensor_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit(self, vid_path, output_json_path, output_video_path=\"\", heatmap=False,\n",
        "            print_prefix=\"\"):\n",
        "\n",
        "        inferer = gaze_inferer(self.model, self.flen, self.ori_video_shape, self.sensor_size)\n",
        "        inferer.process(video_src=vid_path, mode=\"Fit\", batch_size=self.batch_size, output_video_path=output_video_path,\n",
        "                        heatmap=heatmap, print_prefix=print_prefix)\n",
        "        inferer.save_eyeball_model(output_json_path)\n",
        "\n",
        "    def infer(self, vid_path, eyeball_model_path, output_record_path, output_video_path=\"\", heatmap=False,\n",
        "              infer_gaze_flag=True, print_prefix=\"\"):\n",
        "        if isinstance(infer_gaze_flag, str):\n",
        "            try:\n",
        "                infer_gaze_flag = int(infer_gaze_flag)\n",
        "            except ValueError:\n",
        "                infer_gaze_flag = False\n",
        "\n",
        "        inferer = gaze_inferer(self.model, self.flen, self.ori_video_shape, self.sensor_size,\n",
        "                               infer_gaze_flag=infer_gaze_flag)\n",
        "        if infer_gaze_flag:\n",
        "            inferer.load_eyeball_model(eyeball_model_path)\n",
        "        inferer.process(video_src=vid_path, mode=\"Infer\", batch_size=self.batch_size,\n",
        "                        output_record_path=output_record_path, output_video_path=output_video_path, heatmap=heatmap,\n",
        "                        print_prefix=print_prefix)\n",
        "\n",
        "\n",
        "class deepvog_jobman_table_CLI(deepvog_jobman_CLI):\n",
        "    def __init__(self, csv_path, gpu_num, flen, ori_video_shape, sensor_size, batch_size,\n",
        "                 skip_errors=False, skip_existed=False, error_log_path=\"\"):\n",
        "        self.csv_dict = csv_reader(csv_path)\n",
        "        self.skip_errors, self.skip_existed, self.error_log_path = skip_errors, skip_existed, error_log_path\n",
        "        super(deepvog_jobman_table_CLI, self).__init__(gpu_num, flen, ori_video_shape, sensor_size, batch_size)\n",
        "        self._initialize_error_log()\n",
        "\n",
        "    def run_batch(self):\n",
        "        num_operations = len(self.csv_dict['operation'])\n",
        "        operation_counts = dict()\n",
        "        for i in range(num_operations):\n",
        "            current_operation = self.csv_dict['operation'][i]\n",
        "            operation_counts[current_operation] = operation_counts.get(current_operation, 0) + 1\n",
        "\n",
        "        print(\"Total number of operations = %d\" % (num_operations))\n",
        "        print(\"     - Fit    %d/%d \" % (operation_counts.get(\"fit\", 0), num_operations))\n",
        "        print(\"     - Infer  %d/%d \" % (operation_counts.get(\"infer\", 0), num_operations))\n",
        "        print(\"     - Both   %d/%d \" % (operation_counts.get(\"both\", 0), num_operations))\n",
        "        for i in range(num_operations):\n",
        "            try:\n",
        "                current_operation = self.csv_dict['operation'][i]\n",
        "                progress = '%d/%d ' % (i + 1, num_operations)\n",
        "\n",
        "                # Skip file if the output already existed\n",
        "                if self.skip_existed:\n",
        "                    output_existed, concerned_path = self._check_if_output_existed(current_operation, i)\n",
        "                    if output_existed:\n",
        "                        print(\"Video %d skipped (operation %s),\\nbecause %s is/are found\" % (i+1,\n",
        "                                                                                             current_operation,\n",
        "                                                                                             str(concerned_path)))\n",
        "                        continue\n",
        "\n",
        "                # Actions for each operation type\n",
        "                if current_operation == \"fit\":\n",
        "                    self.fit(vid_path=self.csv_dict['fit_vid'][i],\n",
        "                             output_json_path=self.csv_dict['eyeball_model'][i],\n",
        "                             print_prefix=progress)\n",
        "                elif current_operation == \"infer\":\n",
        "                    self.infer(vid_path=self.csv_dict['infer_vid'][i],\n",
        "                               eyeball_model_path=self.csv_dict['eyeball_model'][i],\n",
        "                               output_record_path=self.csv_dict['result'][i],\n",
        "                               infer_gaze_flag=self.csv_dict[\"with_gaze\"][i],\n",
        "                               print_prefix=progress)\n",
        "                elif current_operation == \"both\":\n",
        "                    self.fit(vid_path=self.csv_dict['fit_vid'][i],\n",
        "                             output_json_path=self.csv_dict['eyeball_model'][i],\n",
        "                             print_prefix=progress)\n",
        "                    self.infer(vid_path=self.csv_dict['infer_vid'][i],\n",
        "                               eyeball_model_path=self.csv_dict['eyeball_model'][i],\n",
        "                               output_record_path=self.csv_dict['result'][i],\n",
        "                               infer_gaze_flag=self.csv_dict[\"with_gaze\"][i],\n",
        "                               print_prefix=progress)\n",
        "            except Exception:\n",
        "                existed, paths = self._check_if_output_existed(current_operation, i)\n",
        "                if existed:\n",
        "                    for output_path in paths:\n",
        "                        os.remove(output_path)\n",
        "                if self.skip_errors:\n",
        "                    try:\n",
        "                        print(\"\\nError encountered. Video {} skipped.\".format(i + 1))\n",
        "                        if self.error_log_path:\n",
        "                            self._log_error(i)\n",
        "                        continue\n",
        "                    except Exception:\n",
        "                        print(\"Error encountered when logging the error.\")\n",
        "                        traceback.print_exc()\n",
        "                        continue\n",
        "                else:\n",
        "                    traceback.print_exc()\n",
        "                    raise\n",
        "\n",
        "    def _initialize_error_log(self):\n",
        "        if self.error_log_path:\n",
        "            # This function mainly serves to create and overwrite existing logs\n",
        "            # (plus add some meta information for inspection)\n",
        "            with open(self.error_log_path, \"w\") as fh:\n",
        "                opening_msg = \"DEEPVOG ERROR LOG\\n\"\n",
        "                content = \"Configurations:\\nFLEN:{}\\nOriginal video shape:{}\\nSensor size:{}\\nBatch size:{}\\n\".format(\n",
        "                    self.flen,\n",
        "                    self.ori_video_shape,\n",
        "                    self.sensor_size,\n",
        "                    self.batch_size\n",
        "                )\n",
        "                fh.write(opening_msg + content)\n",
        "\n",
        "    def _log_error(self, i):\n",
        "\n",
        "        with open(self.error_log_path, \"a\") as fh:\n",
        "            msg = [\"{}: {}(delimiter)\".format(k, v[i]) for k, v in self.csv_dict.items()]\n",
        "            msg_pretty = \"\".join(msg).replace(\"(delimiter)\", \"\\n\")\n",
        "            line_separator = \"\\n\" + \"=\"*30 + \"\\n\"\n",
        "            fh.write(line_separator + msg_pretty)\n",
        "            traceback.print_exc(file=fh)\n",
        "        print(\"Error logged in {}\".format(self.error_log_path))\n",
        "\n",
        "    def _check_if_output_existed(self, current_operation, vid_idx):\n",
        "        output_existed = False\n",
        "        concerned_path = \"\"\n",
        "        if current_operation == \"fit\":\n",
        "            # For \"fit\" mode, skip if eyeball model already exists\n",
        "            eyeball_model_path = self.csv_dict['eyeball_model'][vid_idx]\n",
        "            output_existed = os.path.isfile(eyeball_model_path)\n",
        "            concerned_path = (eyeball_model_path, )\n",
        "        elif current_operation == \"infer\":\n",
        "            # For \"infer\" mode, skip if gaze record already exists\n",
        "            output_record_path = self.csv_dict['result'][vid_idx]\n",
        "            output_existed = os.path.isfile(output_record_path)\n",
        "            concerned_path = (output_record_path, )\n",
        "        elif current_operation == \"both\":\n",
        "            # For \"both\" mode, skip if both eyeball AND gaze record already exist\n",
        "            eyeball_model_path = self.csv_dict['eyeball_model'][vid_idx]\n",
        "            output_record_path = self.csv_dict['result'][vid_idx]\n",
        "            eyeball_existed = os.path.isfile(eyeball_model_path)\n",
        "            record_existed = os.path.isfile(output_record_path)\n",
        "            if eyeball_existed and record_existed:\n",
        "                output_existed = True\n",
        "                concerned_path = (eyeball_model_path, output_record_path)\n",
        "        return output_existed, concerned_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-40oMk0yPqqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unprojection.py\n",
        "# The whole unprojection algorithm is invented by Safaee-Rad et al. 1992, see https://ieeexplore.ieee.org/document/163786\n",
        "# This python script is a re-implementation of Safaee-Rad et al.'s works\n",
        "\n",
        "\n",
        "def gen_cone_co(alpha, beta, gamma, a_prime, h_prime, b_prime, g_prime, f_prime, d_prime):\n",
        "    gamma_square = np.power(gamma,2)\n",
        "    a = gamma_square * a_prime\n",
        "    b = gamma_square * b_prime\n",
        "    c = a_prime * np.power(alpha,2) + 2 * h_prime * alpha * beta + b_prime * np.power(beta,2) + 2 * g_prime * alpha + 2 * f_prime * beta + d_prime\n",
        "    d = gamma_square * d_prime\n",
        "    f = -gamma * (b_prime * beta + h_prime * alpha +f_prime)\n",
        "    g = -gamma * (h_prime * beta + a_prime * alpha + g_prime)\n",
        "    h = gamma_square * h_prime\n",
        "    u = gamma_square * g_prime\n",
        "    v = gamma_square * f_prime\n",
        "    w = -gamma * (f_prime * beta + g_prime * alpha + d_prime)\n",
        "    return a,b,c,d,f,g,h,u,v,w\n",
        "\n",
        "'''\n",
        "Safaee-Rad, 1992 (8)\n",
        "'''\n",
        "def gen_rotmat_co(lamb, a,b,g,f,h):\n",
        "    t1 = (b-lamb)*g - f*h\n",
        "    t2 = (a - lamb)*f - g*h\n",
        "    t3 = -(a-lamb)*(t1/t2)/g - (h/g)\n",
        "    m = 1/(np.sqrt(1+np.power((t1/t2),2)+np.power(t3,2)))\n",
        "    l = (t1/t2)*m\n",
        "    n = t3*m\n",
        "    return l, m, n\n",
        "'''\n",
        "Safaee-Rad, 1992 (12), (27)-(33)\n",
        "'''\n",
        "def gen_lmn(lamb1, lamb2, lamb3):\n",
        "    if lamb1 < lamb2:\n",
        "        l = 0\n",
        "        m_pos = np.sqrt((lamb2-lamb1)/(lamb2-lamb3))\n",
        "        m_neg = -m_pos\n",
        "        n = np.sqrt((lamb1-lamb3)/(lamb2-lamb3))\n",
        "        return [l, l], [m_pos, m_neg], [n, n]\n",
        "    elif lamb1 > lamb2:\n",
        "        l_pos = np.sqrt((lamb1-lamb2)/(lamb1-lamb3))\n",
        "        l_neg = -l_pos\n",
        "        n = np.sqrt((lamb2-lamb3)/(lamb1-lamb3))\n",
        "        m = 0\n",
        "        return [l_pos, l_neg], [m, m], [n, n]\n",
        "    elif lamb1 == lamb2:\n",
        "        n = 1\n",
        "        m = 0\n",
        "        l = 0\n",
        "        return [l,l], [m,m], [n,n]\n",
        "    else:\n",
        "        \n",
        "        logging.warning(\"Failure to generate l,m,n. None's are returned\")\n",
        "        return None, None, None\n",
        "def calT3(l, m ,n ):\n",
        "    lm_sqrt = np.sqrt((l**2)+(m**2))\n",
        "    T3 = np.array([-m/lm_sqrt, -(l*n)/lm_sqrt, l, 0,\n",
        "                       l/lm_sqrt, -(m*n)/lm_sqrt, m, 0,\n",
        "                       0, lm_sqrt, n, 0,\n",
        "                       0, 0, 0, 1]).reshape(4,4)\n",
        "    return T3\n",
        "def calABCD(T3, lamb1, lamb2, lamb3):\n",
        "    li, mi, ni = T3[0:3,0], T3[0:3,1], T3[0:3,2]\n",
        "    lamb_array = np.array([lamb1, lamb2, lamb3])\n",
        "    A = np.dot(np.power(li,2), lamb_array)\n",
        "    B = np.sum(li*ni*lamb_array)\n",
        "    C = np.sum(mi*ni*lamb_array)\n",
        "    D = np.dot(np.power(ni,2), lamb_array)\n",
        "    return A,B,C,D\n",
        "def calXYZ_perfect(A,B,C,D, r):\n",
        "    \n",
        "    Z = (A*r)/np.sqrt((B**2)+(C**2)-A*D)\n",
        "    X = (-B/A)*Z\n",
        "    Y = (-C/A)*Z\n",
        "    center = np.array([X,Y,Z,1]).reshape(4,1)\n",
        "    return center\n",
        "    \n",
        "    \n",
        "    \n",
        "def check_parallel(v1, v2):\n",
        "    a = np.dot(v1.T, v2)\n",
        "    b = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
        "    radian = np.arccos(a/b).squeeze()\n",
        "    return np.rad2deg(radian)\n",
        "def convert_ell_to_general(xc,yc, w,h,radian):\n",
        "    A = (w**2)*(np.sin(radian)**2) + (h**2) * (np.cos(radian)**2)\n",
        "    B = 2 * ((h**2) - (w**2)) * np.sin(radian) * np.cos(radian)\n",
        "    C = (w**2)*(np.cos(radian)**2) + (h**2)*(np.sin(radian)**2)\n",
        "    D = -2*A*xc - B*yc\n",
        "    E = -B*xc - 2*C*yc\n",
        "    F = A*(xc**2) + B*xc*yc + C*(yc**2) - (w**2)*(h**2)\n",
        "    return A,B,C,D,E,F\n",
        "\n",
        "\n",
        "def unprojectGazePositions(vertex, ell_co, radius = None):\n",
        "    \"\"\"\n",
        "    This function generates (1)directions of unprojected pupil disk (gaze vector) \n",
        "    and (2) position of the pupil disk, with an assumed radius of the pupil disk\n",
        "    Args:\n",
        "        vectex (list or tuple): list with 3 elements of x, y, z coordinates of the camera with respect to the image frame\n",
        "        ell_co (list or tuple): list of 6 coefficients of a generalised/expanded ellipse equations at the image frame\n",
        "            A*(x**2) + B*x*y + C*(y**2) + D*x + E*y + F = 0 (from https://en.wikipedia.org/wiki/Ellipse#General_ellipse)\n",
        "        \n",
        "    Returns:\n",
        "        Positive Norm of pupil disk from camera frame\n",
        "        Negative Norm of pupil disk from camera frame\n",
        "        Positive Norm of pupil disk from canonical frame\n",
        "        negative Norm of pupil disk from canonical frame\n",
        "    \"\"\"\n",
        "    \n",
        "    # Coefficients of the general ellipse equation\n",
        "    A,B,C,D,E,F = [x for x in ell_co]\n",
        "    # Vertex (Point of the camera)\n",
        "    alpha, beta, gamma = [x for x in vertex]\n",
        "    \n",
        "    # Ellipse parameter at image frame (z_c = +20) with respect to the camera frame\n",
        "    a_prime = A\n",
        "    h_prime = B/2\n",
        "    b_prime = C\n",
        "    g_prime = D/2\n",
        "    f_prime = E/2\n",
        "    d_prime = F\n",
        "    # Coefficients of the Cone at the image frame\n",
        "    a,b,c,d,f,g,h,u,v,w = gen_cone_co(alpha, beta, gamma, a_prime, h_prime, b_prime, g_prime, f_prime, d_prime)\n",
        "    # Safaee-Rad, 1992 (10)\n",
        "    lamb_co1 = 1\n",
        "    lamb_co2 = -(a+b+c)\n",
        "    lamb_co3 = (b*c + c*a + a*b - np.power(f,2) - np.power(g,2) - np.power(h,2))\n",
        "    lamb_co4 = -(a*b*c + 2*f*g*h - a*np.power(f,2) - b*np.power(g,2) - c*np.power(h,2))\n",
        "    lamb1, lamb2, lamb3 = np.roots([lamb_co1, lamb_co2, lamb_co3, lamb_co4])\n",
        "    # generate Normal vector at the canonical frame\n",
        "    \n",
        "    l, m, n = gen_lmn(lamb1,lamb2,lamb3)\n",
        "    norm_cano_pos = np.array([l[0],m[0],n[0],1]).reshape(4,1)\n",
        "    norm_cano_neg = np.array([l[1],m[1],n[1],1]).reshape(4,1)\n",
        "    \n",
        "    # T1 Rotational Transformation to the camera fream\n",
        "    l1, m1, n1 = gen_rotmat_co(lamb1, a,b,g,f,h)\n",
        "    l2, m2, n2 = gen_rotmat_co(lamb2, a,b,g,f,h)\n",
        "    l3, m3, n3 = gen_rotmat_co(lamb3, a,b,g,f,h)\n",
        "    T1 = np.array([l1,l2,l3,0 ,m1, m2, m3,0, n1, n2, n3,0, 0,0,0,1]).reshape(4,4)\n",
        "    li, mi, ni = T1[0,0:3], T1[1,0:3], T1[2,0:3]\n",
        "    if np.cross(li,mi).dot(ni) < 0:\n",
        "        li = -li\n",
        "        mi = -mi\n",
        "        ni = -ni\n",
        "    T1[0,0:3], T1[1,0:3], T1[2,0:3] = li, mi, ni\n",
        "    norm_cam_pos = np.dot(T1,  norm_cano_pos)\n",
        "    norm_cam_neg = np.dot(T1, norm_cano_neg)\n",
        "\n",
        "    # Calculating T2\n",
        "    T2 = np.eye(4)\n",
        "    T2[0:3,3] = -(u*li+v*mi+w*ni)/np.array([lamb1, lamb2, lamb3])\n",
        "    # Calculating T3\n",
        "    T3_pos = calT3(l[0], m[0], n[0])\n",
        "    T3_neg = calT3(l[1], m[1], n[1])\n",
        "    # calculate ABCD\n",
        "    A_pos, B_pos, C_pos, D_pos = calABCD(T3_pos, lamb1, lamb2, lamb3)\n",
        "    A_neg, B_neg, C_neg, D_neg = calABCD(T3_neg, lamb1, lamb2, lamb3)\n",
        "    # Calculating T0\n",
        "    T0 = np.eye(4)\n",
        "\n",
        "    T0[2,3] = -gamma # -gamma = -(vertex[2]) = -(-focal_length) = + focal_length\n",
        "    # Calculating center position with respect to the perfect frame\n",
        "    center_pos = calXYZ_perfect(A_pos, B_pos, C_pos, D_pos, radius)\n",
        "    center_neg = calXYZ_perfect(A_neg, B_neg, C_neg, D_neg, radius)\n",
        "    # From perfect frame to camera frame\n",
        "    true_center_pos = np.matmul(T0,np.matmul(T1,np.matmul(T2,np.matmul(T3_pos,center_pos))))\n",
        "    if true_center_pos[2] <0:\n",
        "        center_pos[0:3] = -center_pos[0:3]\n",
        "        true_center_pos = np.matmul(T0,np.matmul(T1,np.matmul(T2,np.matmul(T3_pos,center_pos))))\n",
        "    true_center_neg = np.matmul(T0,np.matmul(T1,np.matmul(T2,np.matmul(T3_neg,center_neg))))\n",
        "    if true_center_neg[2] <0:\n",
        "        center_neg[0:3] = -center_neg[0:3]\n",
        "        true_center_neg = np.matmul(T0,np.matmul(T1,np.matmul(T2,np.matmul(T3_neg,center_neg))))\n",
        "\n",
        "    return norm_cam_pos[0:3], norm_cam_neg[0:3], true_center_pos[0:3], true_center_neg[0:3]\n",
        "\n",
        "def reproject(vec_3d, focal_length, batch_mode= False):\n",
        "    # vec_3d = (3,1) numpy array: Coordinates of the 3D unprojected object in CAMERA frame\n",
        "    # vec_3d can also be (3,), but not (1,3)\n",
        "    if batch_mode == False:\n",
        "        vec_2d = (focal_length*vec_3d[0:2])/vec_3d[2]\n",
        "        \n",
        "    else:\n",
        "        # converting vec_3d ~ (m,3) to vec_2d~(m,2)\n",
        "        vec_2d = (focal_length*(vec_3d[:,0:2]))/vec_3d[:,[2]]\n",
        "    return vec_2d\n",
        "    \n",
        "def reverse_reproject(vec_2d, z, focal_length):\n",
        "    # Scale the x,y in a reverse manner of reproject() function,\n",
        "    # when you unproject the reprojected coordinate.\n",
        "    vec_2d_scaled = (vec_2d*z)/focal_length\n",
        "    return vec_2d_scaled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rMQOAgZP1ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#utils.py\n",
        "def convert_vec2angle31(n1):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        n1 = numpy array with shape (3,1)\n",
        "    \"\"\"\n",
        "    assert n1.shape == (3,1)\n",
        "    n1 = n1/np.linalg.norm(n1)\n",
        "    n1_x, n1_y, n1_z_abs = n1[0,0], n1[1,0], np.abs(n1[2,0])\n",
        "    # x-augulation            \n",
        "    if n1_x > 0:\n",
        "        x_angle = np.arctan(n1_z_abs/n1_x)\n",
        "    else:\n",
        "        x_angle = np.pi - np.arctan(n1_z_abs/np.abs(n1_x))\n",
        "    # y-angulation\n",
        "    if n1_y > 0:\n",
        "        y_angle = np.arctan(n1_z_abs/n1_y)\n",
        "    else:\n",
        "        y_angle = np.pi - np.arctan(n1_z_abs/np.abs(n1_y))\n",
        "    x_angle = np.rad2deg(x_angle)\n",
        "    y_angle = np.rad2deg(y_angle)\n",
        "    return [x_angle, y_angle]\n",
        "\n",
        "def save_json(path, save_dict):\n",
        "    json_str = json.dumps(save_dict, indent=4)\n",
        "    with open(path, \"w\") as fh:\n",
        "        fh.write(json_str)\n",
        "        \n",
        "def load_json(path):\n",
        "    with open(path, \"r+\") as fh:\n",
        "        json_str = fh.read()\n",
        "    return json.loads(json_str)\n",
        "\n",
        "def csv_reader(csv_path):\n",
        "    col_dict = dict()\n",
        "    col_list = []\n",
        "    with open(csv_path, \"r\") as fh:\n",
        "        for idx, line in enumerate(fh):\n",
        "            row = line.split(\",\")\n",
        "            row_stripped = list(map(lambda x : x.strip(), row))\n",
        "            if idx == 0:\n",
        "                for col in row_stripped:\n",
        "                    col_list.append(col)\n",
        "                    col_dict[str(col)] = []\n",
        "            else:\n",
        "                for col_idx, col in enumerate(row_stripped):\n",
        "                    col_dict[col_list[col_idx]].append(str(col))\n",
        "    return col_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNQPNgH_QCZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualization.py\n",
        "def draw_line(output_frame, frame_shape, o, l, color=[255, 0, 0]):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_frame : numpy.darray\n",
        "        Video frame to draw the circle. The value of video frame should be of type int [0, 255]\n",
        "    frame_shape : list or tuple or numpy.darray\n",
        "        Shape of the frame. For example, (240, 320)\n",
        "    o : list or tuple or numpy.darray\n",
        "        Origin of the line, with shape (2,) denoting (x, y).\n",
        "    l : list or tuple or numpy.darray\n",
        "        Vector with length. Body of the line. Shape = (2, ), denoting (x, y)\n",
        "    color : tuple or list or numpy.darray\n",
        "        RBG colors, e.g. [255, 0, 0] (red color), values of type int [0, 255]\n",
        "    Returns\n",
        "    -------\n",
        "    output frame : numpy.darray\n",
        "        Frame with the ellipse drawn.\n",
        "    \"\"\"\n",
        "    R, G, B = color\n",
        "    rr, cc = line(int(np.round(o[0])), int(np.round(o[1])), int(np.round(o[0] + l[0])), int(np.round(o[1] + l[1])))\n",
        "    rr[rr > int(frame_shape[1]) - 1] = frame_shape[1] - 1\n",
        "    cc[cc > int(frame_shape[0]) - 1] = frame_shape[0] - 1\n",
        "    rr[rr < 0] = 0\n",
        "    cc[cc < 0] = 0\n",
        "    output_frame[cc, rr, 0] = R\n",
        "    output_frame[cc, rr, 1] = G\n",
        "    output_frame[cc, rr, 2] = B\n",
        "    return output_frame\n",
        "\n",
        "\n",
        "def draw_ellipse(output_frame, frame_shape, ellipse_info, color=[255, 255, 0]):\n",
        "    \"\"\"\n",
        "    Draw a circle on an image or video frame. Drawing will be discretized.\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_frame : numpy.darray\n",
        "        Video frame to draw the circle. The value of video frame should be of type int [0, 255]\n",
        "    frame_shape : list or tuple or numpy.darray\n",
        "        Shape of the frame. For example, (240, 320)\n",
        "    ellipse_info : list or tuple\n",
        "        Information of ellipse parameters. (rr, cc, centre, w, h, radian, ellipse_confidence).\n",
        "    color : tuple or list or numpy.darray\n",
        "        RBG colors, e.g. [255, 0, 0] (red color), values of type int [0, 255]\n",
        "    Returns\n",
        "    -------\n",
        "    output frame : numpy.darray\n",
        "        Frame withe the ellipse drawn.\n",
        "    \"\"\"\n",
        "\n",
        "    R, G, B = color\n",
        "    (rr, cc, centre, w, h, radian, ellipse_confidence) = ellipse_info\n",
        "    rr[rr > int(frame_shape[1]) - 1] = frame_shape[1] - 1\n",
        "    cc[cc > int(frame_shape[0]) - 1] = frame_shape[0] - 1\n",
        "    rr[rr < 0] = 0\n",
        "    cc[cc < 0] = 0\n",
        "    output_frame[cc, rr, 0] = R\n",
        "    output_frame[cc, rr, 1] = G\n",
        "    output_frame[cc, rr, 2] = B\n",
        "    return output_frame\n",
        "\n",
        "\n",
        "def draw_circle(output_frame, frame_shape, centre, radius, color=[255, 0, 0]):\n",
        "    \"\"\"\n",
        "    Draw a circle on an image or video frame. Drawing will be discretized.\n",
        "    Parameters\n",
        "    ----------\n",
        "    output_frame : numpy.darray\n",
        "        Video frame to draw the circle. The value of video frame should be of type int [0, 255]\n",
        "    frame_shape : list or tuple or numpy.darray\n",
        "        Shape of the frame. For example, (240, 320)\n",
        "    centre : list or tuple or numpy.darray\n",
        "        x,y coordinate of the circle centre\n",
        "    radius : int or float\n",
        "        Radius of the circle to draw.\n",
        "    color : tuple or list or numpy.darray\n",
        "        RBG colors, e.g. [255, 0, 0] (red color), values of type int [0, 255]\n",
        "    Returns\n",
        "    -------\n",
        "    output frame : numpy.darray\n",
        "        Frame withe the circle drawn.\n",
        "    \"\"\"\n",
        "\n",
        "    R, G, B = color\n",
        "    rr_p1, cc_p1 = circle_perimeter(int(np.round(centre[0])), int(np.round(centre[1])), radius)\n",
        "    rr_p1[rr_p1 > int(frame_shape[1]) - 1] = frame_shape[1] - 1\n",
        "    cc_p1[cc_p1 > int(frame_shape[0]) - 1] = frame_shape[0] - 1\n",
        "    rr_p1[rr_p1 < 0] = 0\n",
        "    cc_p1[cc_p1 < 0] = 0\n",
        "    output_frame[cc_p1, rr_p1, 0] = R\n",
        "    output_frame[cc_p1, rr_p1, 1] = G\n",
        "    output_frame[cc_p1, rr_p1, 2] = B\n",
        "    return output_frame\n",
        "\n",
        "\n",
        "class VideoManager:\n",
        "    def __init__(self, vreader, output_record_path=\"\", output_video_path=\"\", heatmap=False):\n",
        "        # Parameters\n",
        "        self.vreader = vreader\n",
        "        self.heatmap = heatmap\n",
        "        self.output_video_flag = True if output_video_path else False\n",
        "        self.output_record_flag = True if output_record_path else False\n",
        "        self.vwriter = skv.FFmpegWriter(output_video_path) if self.output_video_flag else None\n",
        "        self.results_recorder = open(output_record_path, \"w\") if self.output_record_flag else None\n",
        "\n",
        "        # Initialization actions\n",
        "        self._initialize_results_recorder()\n",
        "\n",
        "    def write_frame_with_condition(self, vid_frame, pred_each):\n",
        "\n",
        "        if self.heatmap:\n",
        "            heatmap_frame = np.zeros((pred_each.shape[0], pred_each.shape[1], 3))  # Shape = (w, h, 3)\n",
        "            heatmap_frame[:, :, :] = np.around(\n",
        "                pred_each.reshape(pred_each.shape[0], pred_each.shape[1], 1) * 255).astype(int)\n",
        "            output_frame = np.concatenate((vid_frame, heatmap_frame), axis=1)\n",
        "            self.vwriter.writeFrame(output_frame)\n",
        "        else:\n",
        "            self.vwriter.writeFrame(vid_frame)\n",
        "\n",
        "    def write_results(self, frame_id, pupil2D_x, pupil2D_y, gaze_x, gaze_y, confidence, consistence):\n",
        "        self.results_recorder.write(\"%d,%f,%f,%f,%f,%f,%f\\n\" % (frame_id, pupil2D_x, pupil2D_y,\n",
        "                                                                gaze_x, gaze_y,\n",
        "                                                                confidence, consistence))\n",
        "\n",
        "    def _initialize_results_recorder(self):\n",
        "        if self.output_record_flag:\n",
        "            self.results_recorder.write(\"frame,pupil2D_x,pupil2D_y,gaze_x,gaze_y,confidence,consistence\\n\")\n",
        "\n",
        "    def __del__(self):\n",
        "        self.vreader.close()\n",
        "        if self.vwriter:\n",
        "            self.vwriter.close()\n",
        "        if self.results_recorder:\n",
        "            self.results_recorder.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJmp5PceQtUr",
        "colab_type": "text"
      },
      "source": [
        "#main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Mzz2qeQnI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7da259d9-88a8-4c53-e64d-2e08f5c56584"
      },
      "source": [
        "import argparse\n",
        "from argparse import RawTextHelpFormatter\n",
        "from ast import literal_eval\n",
        "\n",
        "# Running DeepVOG with command-line parser.\n",
        "\n",
        "description_text = \"\"\"\n",
        "Belows are the examples of usage. Don't forget to set up camera parameters such as focal length, because it varies from equipment to equipment and is necessaray for accuracy.\n",
        "    Example #1 - Fitting an eyeball model (using default camera parameters)\n",
        "    python -m deepvog --fit ~/video01.mp4 ~/model01.json\n",
        "    Example #2 - Infer gaze (using default camera parameters)\n",
        "    python -m deepvog --infer ~/video01.mp4 ~/model01.json ~/model1_video01.csv\n",
        "    Example #3 - Setting up necessary parameters (focal length = 12mm, video shape = (240,320), ... etc)\n",
        "    python -m deepvog --fit ./vid.mp4 ./model.json -f 12 -vs 240 320 -s 3.6 4.8 -b 32 -g 0\n",
        "Caution: If your video has an aspect ratio (height/width) other than 0.75, please crop it manually until it reaches the required aspect ratio otherwise the gaze estimate will not be accurate. Futher release will include cropping option.\n",
        "    Example #4 - If you cropped the video from (250,390) to (240,360), keep using the argument \"-vs 250 390\"\n",
        "    python -m deepvog --fit ./vid.mp4 ./model.json -f 12 -vs 250 390 -s 3.6 4.8 -b 32 -g 0\n",
        "    \n",
        "You can also fit and infer a video from a csv table:\n",
        "    Example #5\n",
        "    python -m deepvog --table ./table.csv -f 12 -vs 240 320 -s 3.6 4.8 -b 32 -g 0\n",
        "The csv file must follow the format below:\n",
        "    \n",
        "    operation, fit_vid,             infer_vid,              eyeball_model,      result\n",
        "    fit      , /PATH/fit_vid.mp4,   /PATH/infer_vid.mp4,    /PATH/model.json,   /PATH/output_result.csv\n",
        "    infer    , /PATH/fit_vid2.mp4,  /PATH/infer_vid2.mp4,   /PATH/model2.json,  /PATH/output_result2.csv\n",
        "    both     , /PATH/fit_vid3.mp4,  /PATH/infer_vid3.mp4,   /PATH/model3.json,  /PATH/output_result3.csv\n",
        "    fit      , /PATH/fit_vid4.mp4,  /PATH/infer_vid4.mp4,   /PATH/model4.json,  /PATH/output_result4.csv\n",
        "    ...\n",
        "The \"operation\" column should contain either fit/infer/both: \n",
        "    1. fit: it will fit the video specified by the path \"fit_vid\" and save the model to the path specified by \"eyeball_model\". Other columns are irrelevant and can be omited.\n",
        "    2. infer: it will load the model from the path specified in \"eyeball_model\", and infer the video from the path specified in \"infer_vid\", and then save the result to the path specified by \"result\".\n",
        "    3. both: first performs \"fit\", then performs \"infer\".\n",
        "\"\"\"\n",
        "'''\n",
        "fit_help = \"Fitting an eyeball model. Call with --fit [video_src_path] [eyeball_model_saving_path].\"\n",
        "infer_help = \"Inter video from eyeball model. Call with --infer [video_scr_path] [eyeball_model_path] [results_saving_path]\"\n",
        "table_help = \"Fit or infer videos from a csv table. The column names of the csv must follow a format (see --help description). Call with --table [csv_path]\"\n",
        "ori_vid_shape_help = \"Original and uncropped video shape of your camera output, height and width in pixel. Default = 240 320\"\n",
        "flen_help = \"Focal length of your camera in mm.\"\n",
        "gpu_help = \"GPU device number. Default = 0\"\n",
        "sensor_help = \"Sensor size of your camera digital sensor, height and width in mm. Default = 3.6 4.8\"\n",
        "batch_help = \"Batch size for forward inference. Default = 512.\"\n",
        "visualize_help = \"Draw the visualization of ellipse fitting and gaze vector. Call with --visualize [video_output_path]. (Not yet available with --table mode)\"\n",
        "heatmap_help = \"Show network's output of segmented pupil heatmap in visualization. Call with --heatmap. (Not yet available with --table mode)\"\n",
        "\n",
        "parser = argparse.ArgumentParser(description=description_text, formatter_class=RawTextHelpFormatter)\n",
        "required = parser.add_argument_group(\"required arguments\")\n",
        "required.add_argument(\"--fit\", help=fit_help, nargs=2, type=str, metavar=(\"PATH\", \"PATH\"))\n",
        "required.add_argument(\"--infer\", help=infer_help, nargs=3, type=str, metavar=(\"PATH\", \"PATH\", \"PATH\"))\n",
        "required.add_argument(\"--table\", help=table_help, type=str, metavar=(\"PATH\"))\n",
        "parser.add_argument(\"-f\", \"--flen\", help=flen_help, default=6, type=float, metavar=(\"FLOAT\"))\n",
        "parser.add_argument(\"-g\", \"--gpu\", help=gpu_help, default=\"0\", type=str, metavar=(\"INT\"))\n",
        "parser.add_argument(\"-vs\", \"--vidshape\", help=ori_vid_shape_help, default=\"(240, 320)\", type=str, metavar=(\"INT,INT\"))\n",
        "parser.add_argument(\"-s\", \"--sensor\", help=sensor_help, default=\"(3.6, 4.8)\", type=str, metavar=(\"FLOAT,FLOAT\"))\n",
        "parser.add_argument(\"-b\", \"--batchsize\", help=batch_help, default=512, type=int, metavar=(\"INT\"))\n",
        "parser.add_argument(\"-v\", \"--visualize\", help=visualize_help, default=\"\", type=str, metavar=(\"PATH\"))\n",
        "parser.add_argument(\"-m\", \"--heatmap\", help=heatmap_help, default=False, action=\"store_true\")\n",
        "parser.add_argument(\"--skip_existed\", default=False, action=\"store_true\")\n",
        "parser.add_argument(\"--skip_errors\", default=False, action=\"store_true\")\n",
        "parser.add_argument(\"--log_errors\", type=str, default=\"\", metavar=(\"PATH\"))\n",
        "parser.add_argument(\"--no_gaze\", default=True, action=\"store_false\")\n",
        "args = parser.parse_args()\n",
        "'''\n",
        "\n",
        "# Check there is EXACTLY one argument from --fit, --infer and --table\n",
        "all_modes_list = [args.fit, args.infer, args.table]\n",
        "cli_modes_list = all_modes_list[0:3]\n",
        "num_modes = sum([x is not None for x in all_modes_list])\n",
        "if num_modes != 1:\n",
        "    parser.error(\"Exactly one argument from --fit, --infer and --table is requried\")\n",
        "else:\n",
        "\n",
        "    # Command line mode\n",
        "    if sum([x is not None for x in cli_modes_list]) > 0:\n",
        "\n",
        "        from .jobman import deepvog_jobman_CLI, deepvog_jobman_table_CLI\n",
        "\n",
        "        flen = args.flen\n",
        "        gpu = args.gpu\n",
        "        ori_video_shape = literal_eval(args.vidshape)\n",
        "        sensor_size = literal_eval(args.sensor)\n",
        "        batch_size = args.batchsize\n",
        "\n",
        "        # Table mode\n",
        "        if args.table is not None:\n",
        "            jobman_table = deepvog_jobman_table_CLI(args.table, gpu, flen,\n",
        "                                                    ori_video_shape, sensor_size, batch_size,\n",
        "                                                    skip_errors=args.skip_errors,\n",
        "                                                    skip_existed=args.skip_existed,\n",
        "                                                    error_log_path=args.log_errors)\n",
        "            jobman_table.run_batch()\n",
        "\n",
        "        # Fit or Infer mode\n",
        "        jobman = deepvog_jobman_CLI(gpu, flen, ori_video_shape, sensor_size, batch_size)\n",
        "        if args.fit is not None:\n",
        "            vid_src_fitting, eyemodel_save = args.fit\n",
        "            jobman.fit(vid_path=vid_src_fitting, output_json_path=eyemodel_save, output_video_path=args.visualize,\n",
        "                       heatmap=args.heatmap)\n",
        "        if args.infer is not None:\n",
        "            vid_scr_inference, eyemodel_load, result_output = args.infer\n",
        "            jobman.infer(vid_path=vid_scr_inference,\n",
        "                         eyeball_model_path=eyemodel_load,\n",
        "                         output_record_path=result_output,\n",
        "                         output_video_path=args.visualize,\n",
        "                         heatmap=args.heatmap,\n",
        "                         infer_gaze_flag=args.no_gaze)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c275b291416c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Check there is EXACTLY one argument from --fit, --infer and --table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mall_modes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mcli_modes_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_modes_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mnum_modes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_modes_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    }
  ]
}